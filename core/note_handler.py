import os
import json
import time
import logging
import requests
import re
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import google.generativeai as genai

class NoteHandler:
    """
    ç­†è¨˜è™•ç†å™¨é¡åˆ¥
    è² è²¬è™•ç†æ‘˜è¦ç”Ÿæˆã€ç­†è¨˜æ ¼å¼åŒ–å’Œ Notion æ•´åˆ
    """
    
    def __init__(self, notion_formatter):
        """
        åˆå§‹åŒ–ç­†è¨˜è™•ç†å™¨
        
        Args:
            notion_formatter: Notion æ ¼å¼åŒ–å·¥å…·å¯¦ä¾‹
        """
        self.notion_formatter = notion_formatter
        self.notion_token = os.getenv("NOTION_TOKEN")
        self.notion_database_id = os.getenv("NOTION_DATABASE_ID")
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        
        # é…ç½® Google Gemini API
        if self.gemini_api_key:
            genai.configure(api_key=self.gemini_api_key)
        else:
            logging.warning("âš ï¸ æœªè¨­ç½® GEMINI_API_KEY ç’°å¢ƒè®Šæ•¸ï¼ŒAI åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")

    def try_multiple_gemini_models(self, system_prompt: str, user_content: str, 
                                models: Optional[List[str]] = None) -> Any:
        """
        å˜—è©¦ä½¿ç”¨å¤šå€‹ Gemini æ¨¡å‹ï¼Œç›´åˆ°ä¸€å€‹æˆåŠŸ
        
        Args:
            system_prompt: ç³»çµ±æç¤ºè©
            user_content: ä½¿ç”¨è€…å…§å®¹
            models: è¦å˜—è©¦çš„æ¨¡å‹åˆ—è¡¨ï¼Œå¦‚æœç‚º None å‰‡ä½¿ç”¨é è¨­åˆ—è¡¨
            
        Returns:
            Gemini API å›æ‡‰å°è±¡
            
        Raises:
            ValueError: è‹¥æ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—
        """
        # é è¨­æ¨¡å‹åˆ—è¡¨
        if models is None:
            models = [
                'gemini-2.5-pro-exp-03-25', 
                'gemini-1.5-pro', 
                'gemini-2.5-flash-preview-04-17',
                'gemini-2.0-flash', 
                'gemini-1.5-flash', 
                'gemini-2.0-flash-lite'
            ]
        
        response = None
        last_error = None
        quota_errors = []

        # å˜—è©¦ä¸åŒæ¨¡å‹ç›´åˆ°æˆåŠŸ
        for model_name in models:
            try:
                logging.info(f"ğŸ”„ ä½¿ç”¨æ¨¡å‹: {model_name}")
                model = genai.GenerativeModel(model_name)
                
                # é…ç½®ç”Ÿæˆåƒæ•¸
                generation_config = {
                    "temperature": 0.2,  # è¼ƒä½æº«åº¦ä»¥ç²å¾—æ›´ä¸€è‡´çš„å›æ‡‰
                    "top_p": 0.9,
                    "top_k": 40,
                    "max_output_tokens": 4096
                }
                
                # ç”Ÿæˆå…§å®¹
                response = model.generate_content(
                    [system_prompt, user_content],
                    generation_config=generation_config
                )
                
                # æˆåŠŸå‰‡è·³å‡ºå¾ªç’°
                logging.info(f"âœ… æˆåŠŸä½¿ç”¨æ¨¡å‹ {model_name}")
                break
                
            except Exception as e:
                last_error = e
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºé…é¡éŒ¯èª¤
                if "429" in str(e) or "quota" in str(e).lower():
                    # æå–éŒ¯èª¤è¨Šæ¯ä¸­çš„ URL
                    url_match = re.search(r'https?://\S+', str(e))
                    url_info = url_match.group(0) if url_match else "æœªçŸ¥URL"
                    
                    logging.warning(f"âš ï¸ æ¨¡å‹ {model_name} é…é¡å·²ç”¨ç›¡: {url_info}")
                    quota_errors.append(model_name)
                    continue
                else:
                    # å…¶ä»–éé…é¡éŒ¯èª¤
                    logging.error(f"âŒ ä½¿ç”¨æ¨¡å‹ {model_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    if model_name == models[-1]:
                        raise  # æœ€å¾Œä¸€å€‹æ¨¡å‹ä¹Ÿå¤±æ•—æ™‚æ‰æ‹‹å‡ºéŒ¯èª¤

        # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—
        if response is None:
            if quota_errors:
                error_msg = f"æ‰€æœ‰æ¨¡å‹éƒ½é”åˆ°é…é¡é™åˆ¶: {', '.join(quota_errors)}"
            else:
                error_msg = f"æ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—: {str(last_error)}"
                
            logging.error(f"âŒ {error_msg}")
            raise ValueError(error_msg)
        
        return response

    def generate_comprehensive_notes(self, transcript: str) -> str:
        """
        ä½¿ç”¨ Gemini ç”Ÿæˆçµæ§‹åŒ–ç­†è¨˜
        
        Args:
            transcript: æœƒè­°é€å­—ç¨¿
            
        Returns:
            Markdown æ ¼å¼çš„çµæ§‹åŒ–ç­†è¨˜
        """
        logging.info("ğŸ”„ ç”Ÿæˆæœƒè­°ç­†è¨˜...")
        
        try:
            # ç³»çµ±æç¤ºè©
            system_prompt = """
            ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æœƒè­°ç­†è¨˜æ•´ç†å°ˆå®¶ã€‚è«‹å°‡æä¾›çš„æœƒè­°é€å­—ç¨¿æ•´ç†ç‚ºçµæ§‹åŒ–çš„ Markdown æ ¼å¼ç­†è¨˜ã€‚
            
            è«‹éµå¾ªä»¥ä¸‹æ ¼å¼:
            1. ä½¿ç”¨é©ç•¶çš„æ¨™é¡Œå±¤ç´š (##, ###)
            2. è­˜åˆ¥ä¸¦åˆ—å‡ºé—œéµä¸»é¡Œã€æ±ºç­–å’Œè¡Œå‹•é …ç›®
            3. ä¾ç…§é‚è¼¯é †åºçµ„ç¹”å…§å®¹ï¼Œå°‡ç›¸é—œè¨è«–åˆ†çµ„
            4. ä½¿ç”¨æ¸…å–® (- æˆ– 1.) ä¾†åˆ—èˆ‰è¦é»
            5. ä½¿ç”¨ç²—é«”å’Œæ–œé«”ä¾†å¼·èª¿é‡è¦å…§å®¹
            
            ç›´æ¥è¼¸å‡º Markdownï¼Œä¸è¦ä½¿ç”¨ä»£ç¢¼å€å¡Šæ¨™è¨˜å¦‚ ```markdownã€‚
            ä¿æŒå®¢è§€ã€ç°¡æ½”ï¼Œä¸è¦æ·»åŠ æœªåœ¨åŸå§‹å…§å®¹ä¸­æåŠçš„è§£é‡‹æˆ–è³‡è¨Šã€‚
            """

            # ä½¿ç”¨å‚™é¸æ¨¡å‹å˜—è©¦ç”Ÿæˆ
            response = self.try_multiple_gemini_models(
                system_prompt,
                f"æœƒè­°é€å­—ç¨¿ï¼š\n{transcript}",
                models=['gemini-2.5-flash-preview-04-17', 'gemini-1.5-pro', 'gemini-2.0-flash-lite']
            )
            
            # æå–ç­†è¨˜å…§å®¹
            notes = response.text.strip()
            logging.info("âœ… ç­†è¨˜ç”ŸæˆæˆåŠŸ")
            return notes
            
        except Exception as e:
            logging.error(f"âŒ ç­†è¨˜ç”Ÿæˆå¤±æ•—: {str(e)}")
            return f"### ç­†è¨˜ç”Ÿæˆå¤±æ•—\n\nç™¼ç”ŸéŒ¯èª¤: {str(e)}\n\nè«‹åƒè€ƒå®Œæ•´é€å­—ç¨¿ã€‚"
    
    def generate_summary(self, transcript: str, attachment_text: Optional[str] = None) -> Dict[str, Any]:
        """
        ä½¿ç”¨ Gemini ç”Ÿæˆæ‘˜è¦ã€æ¨™é¡Œå’Œå¾…è¾¦äº‹é …
        
        Args:
            transcript: æœƒè­°é€å­—ç¨¿
            attachment_text: é™„ä»¶æ–‡æœ¬ï¼ˆå¯é¸ï¼‰
            
        Returns:
            åŒ…å«æ¨™é¡Œã€æ‘˜è¦å’Œå¾…è¾¦äº‹é …çš„å­—å…¸
        """
        logging.info("ğŸ”„ ç”Ÿæˆæœƒè­°æ‘˜è¦...")
        
        try:
            # æº–å‚™ä¸Šä¸‹æ–‡
            context = ""
            if attachment_text:
                context = f"ä»¥ä¸‹æ˜¯æä¾›çš„èƒŒæ™¯è³‡æ–™ï¼š\n{attachment_text[:3000]}...\n\n" if len(attachment_text) > 3000 else f"ä»¥ä¸‹æ˜¯æä¾›çš„èƒŒæ™¯è³‡æ–™ï¼š\n{attachment_text}\n\n"
            
            # æˆªå–é€å­—ç¨¿ï¼Œé¿å…å¤ªé•·
            truncated_transcript = transcript
            if len(transcript) > 12000:  # ç´„ 3000 å€‹ä¸­æ–‡å­—
                truncated_transcript = transcript[:12000] + "...(å¾ŒçºŒå…§å®¹å·²çœç•¥)"
                
            # ç³»çµ±æç¤ºè©
            system_prompt = """
            ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æœƒè­°åˆ†æå°ˆå®¶ï¼Œç²¾é€šæ‘˜è¦ç”Ÿæˆå’Œè­˜åˆ¥è¡Œå‹•é …ç›®ã€‚
            è«‹åˆ†ææä¾›çš„æœƒè­°è¨˜éŒ„ï¼Œä¸¦ä»¥ JSON æ ¼å¼è¿”å›ä»¥ä¸‹å…§å®¹:
            
            1. title: ç°¡çŸ­ä¸”æè¿°æ€§çš„æœƒè­°æ¨™é¡Œ (20å­—ä»¥å…§)
            2. summary: æœƒè­°æ ¸å¿ƒå…§å®¹çš„æ‘˜è¦ (200-300å­—)
            3. todos: æœƒè­°ä¸­æ˜ç¢ºæåˆ°æˆ–éš±å«çš„å¾…è¾¦äº‹é …åˆ—è¡¨ (æ¯é …æœ€å¤š30å­—)
            
            JSON æ ¼å¼ç¯„ä¾‹:
            {
              "title": "æœƒè­°æ¨™é¡Œ",
              "summary": "æœƒè­°æ‘˜è¦...",
              "todos": ["å¾…è¾¦äº‹é …1", "å¾…è¾¦äº‹é …2", "..."]
            }
            
            åƒ…è¿”å›ç´” JSONï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚å¦‚æœæ‰¾ä¸åˆ°å¾…è¾¦äº‹é …ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
            """
            
            # ä½¿ç”¨å‚™é¸æ¨¡å‹ç”Ÿæˆ
            response = self.try_multiple_gemini_models(
                system_prompt,
                f"{context}æœƒè­°è¨˜éŒ„ï¼š\n{truncated_transcript}"
            )
            
            # è§£æ JSON
            response_text = response.text
            json_match = re.search(r'({.*?})', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
                
            try:
                summary_data = json.loads(response_text)
                
                # ç¢ºä¿æ‰€æœ‰å­—æ®µéƒ½å­˜åœ¨
                summary_data.setdefault("title", "æœƒè­°è¨˜éŒ„")
                summary_data.setdefault("summary", "æœªèƒ½ç”Ÿæˆæ‘˜è¦")
                summary_data.setdefault("todos", [])
                
                logging.info(f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸï¼š{summary_data['title']}")
                return summary_data
                
            except json.JSONDecodeError as e:
                logging.error(f"âŒ ç„¡æ³•è§£æ AI å›æ‡‰çš„ JSON: {str(e)}")
                return {
                    "title": "æœƒè­°è¨˜éŒ„",
                    "summary": "æ‘˜è¦ç”Ÿæˆå¤±æ•—ã€‚ç„¡æ³•è§£æ AI å›æ‡‰ã€‚",
                    "todos": ["æª¢æŸ¥æ‘˜è¦ç”Ÿæˆæœå‹™"]
                }
                
        except Exception as e:
            logging.error(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±æ•—: {str(e)}")
            return {
                "title": "æœƒè­°è¨˜éŒ„",
                "summary": f"æ‘˜è¦ç”Ÿæˆå¤±æ•—: {str(e)}",
                "todos": ["æª¢æŸ¥æ‘˜è¦ç”Ÿæˆæœå‹™"]
            }
    
    def identify_speakers(self, segments: List[Dict[str, Any]], original_speakers: List[str]) -> Dict[str, str]:
        """
        å˜—è©¦è¾¨è­˜èªªè©±äººèº«ä»½
        
        Args:
            segments: èªéŸ³æ®µè½åˆ—è¡¨
            original_speakers: åŸå§‹èªªè©±äººç·¨è™Ÿåˆ—è¡¨
            
        Returns:
            èªªè©±äººæ˜ å°„å­—å…¸ï¼Œå¾åŸå§‹ç·¨è™Ÿåˆ°çŒœæ¸¬çš„èº«ä»½
        """
        logging.info("ğŸ”„ è¾¨è­˜èªªè©±äººèº«ä»½...")
        
        try:
            # å¦‚æœåªæœ‰ä¸€ä½èªªè©±äººï¼Œç›´æ¥è¿”å›
            if len(original_speakers) <= 1:
                return {original_speakers[0]: "åƒèˆ‡è€… 1"} if original_speakers else {}
            
            # å»ºç«‹åˆå§‹è©±è€…æ˜ å°„
            speaker_map = {speaker: f"åƒèˆ‡è€… {i+1}" for i, speaker in enumerate(original_speakers)}
            
            # ç‚ºæ¯ä½èªªè©±äººæ”¶é›†ç™¼è¨€æ¨£æœ¬
            speaker_samples = {}
            for segment in segments:
                speaker = segment["speaker"]
                text = segment["text"]
                
                if not text.strip():
                    continue
                    
                if speaker not in speaker_samples:
                    speaker_samples[speaker] = []
                    
                if len(speaker_samples[speaker]) < 5:  # æ¯ä½èªªè©±äººæœ€å¤šæ”¶é›† 5 å€‹æ¨£æœ¬
                    speaker_samples[speaker].append(text)
            
            # å¦‚æœæ²’æœ‰è¶³å¤ æ¨£æœ¬ï¼Œä½¿ç”¨é»˜èªæ˜ å°„
            if not speaker_samples or sum(len(samples) for samples in speaker_samples.values()) < 3:
                return speaker_map
            
            # æº–å‚™æç¤ºè©
            sample_text = ""
            for speaker, samples in speaker_samples.items():
                sample_text += f"\nã€{speaker} çš„ç™¼è¨€ç¯„ä¾‹ã€‘\n"
                for i, sample in enumerate(samples):
                    sample_text += f"{i+1}. {sample}\n"
            
            system_prompt = """
            ä½ æ˜¯ä¸€ä½å°è©±åˆ†æå°ˆå®¶ï¼Œèƒ½å¾å°è©±å…§å®¹æ¨æ¸¬èªªè©±è€…çš„èº«ä»½æˆ–è§’è‰²ã€‚
            è«‹åˆ†ææä¾›çš„å°è©±ç‰‡æ®µï¼Œæ¨æ¸¬æ¯ä½èªªè©±è€…å¯èƒ½çš„èº«ä»½æˆ–è§’è‰²ï¼ˆå¦‚ï¼šä¸»æŒäººã€å°ˆæ¡ˆç¶“ç†ã€å®¢æˆ¶ç­‰ï¼‰ã€‚
            å¦‚ç„¡æ³•åˆ¤æ–·å…·é«”èº«ä»½ï¼Œå¯ç”¨ã€Œåƒèˆ‡è€… 1ã€ã€ã€Œåƒèˆ‡è€… 2ã€ç­‰è¡¨ç¤ºã€‚

            è«‹ä»¥ JSON æ ¼å¼è¿”å›çµæœ:
            {"SPEAKER_ID": "èº«ä»½åç¨±", ...}

            åªè¿”å› JSON æ ¼å¼ï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚
            """
            
            response = self.try_multiple_gemini_models(
                system_prompt,
                f"ä»¥ä¸‹æ˜¯ä¸€æ®µæœƒè­°ä¸­ä¸åŒè¬›è€…çš„ç™¼è¨€æ¨£æœ¬ï¼Œè«‹å¹«åŠ©è­˜åˆ¥ä»–å€‘å¯èƒ½çš„èº«ä»½æˆ–è§’è‰²ï¼š{sample_text}",
                models=['gemini-2.0-flash', 'gemini-1.5-flash', 'gemini-2.0-flash-lite']
            )
            
            # è§£æçµæœ
            try:
                # å˜—è©¦å¾å›æ‡‰ä¸­æå– JSON
                json_match = re.search(r'{.*}', response.text, re.DOTALL)
                if json_match:
                    speaker_identities = json.loads(json_match.group(0))
                    
                    # ä½¿ç”¨è­˜åˆ¥çµæœæ›´æ–°æ˜ å°„
                    for speaker in original_speakers:
                        if speaker in speaker_identities and speaker_identities[speaker].strip():
                            speaker_map[speaker] = speaker_identities[speaker]
                    
                    logging.info(f"âœ… è­˜åˆ¥åˆ°çš„èªªè©±äºº: {speaker_map}")
                    return speaker_map
                else:
                    logging.warning("âš ï¸ ç„¡æ³•å¾å›æ‡‰ä¸­æå– JSON")
                    return speaker_map
                    
            except Exception as e:
                logging.warning(f"âš ï¸ è§£æèªªè©±äººèº«ä»½æ™‚å‡ºéŒ¯: {str(e)}")
                return speaker_map
                
        except Exception as e:
            logging.error(f"âŒ èªªè©±äººèº«ä»½è­˜åˆ¥å¤±æ•—: {str(e)}")
            return {speaker: f"åƒèˆ‡è€… {i+1}" for i, speaker in enumerate(original_speakers)}
    
    def create_notion_page(self, title: str, summary: str, todos: List[str], 
                          segments: List[Dict[str, Any]], speaker_map: Dict[str, str], 
                          google_service, file_id: str = None) -> Tuple[str, str]:
        """
        å»ºç«‹ Notion é é¢
        
        Args:
            title: é é¢æ¨™é¡Œ
            summary: æ‘˜è¦å…§å®¹
            todos: å¾…è¾¦äº‹é …åˆ—è¡¨
            segments: èªéŸ³æ®µè½åˆ—è¡¨
            speaker_map: èªªè©±äººæ˜ å°„
            google_service: Google æœå‹™å¯¦ä¾‹ï¼Œç”¨æ–¼ç²å–æª”æ¡ˆé€£çµ
            file_id: Google Drive æª”æ¡ˆ ID
            
        Returns:
            Tuple[str, str]: (Notion é é¢ ID, é é¢ URL)
        """
        logging.info("ğŸ”„ å»ºç«‹ Notion é é¢...")

        if not self.notion_token or not self.notion_database_id:
            raise ValueError("ç¼ºå°‘ Notion API è¨­å®š (NOTION_TOKEN æˆ– NOTION_DATABASE_ID)")

        # --- æº–å‚™é é¢å…§å®¹å€å¡Š ---
        blocks = []
        
        # å¾æª”æ¡ˆåç¨±æå–æ—¥æœŸæˆ–ä½¿ç”¨ç•¶å‰æ—¥æœŸ
        current_date = datetime.now()
        date_str = current_date.strftime("%Y-%m-%d")
        
        # å˜—è©¦å¾æª”æ¡ˆIDæå–æ—¥æœŸ
        if file_id and google_service:
            try:
                file_details = google_service.get_file_details(file_id)
                file_name = file_details.get("name", "")
                extracted_date = google_service.extract_date_from_filename(file_name)
                if extracted_date:
                    date_str = extracted_date
            except Exception as e:
                logging.error(f"âŒ æå–æª”æ¡ˆæ—¥æœŸå¤±æ•—: {str(e)}")
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        formatted_date = datetime.strptime(date_str, "%Y-%m-%d").strftime("%Yå¹´%mæœˆ%dæ—¥")

        # --- æ¨™é¡Œå€å¡Š ---
        page_title = f"{formatted_date} {title}"

        # --- æ—¥æœŸå€å¡Š ---
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "ğŸ“… æ—¥æœŸ"}}]
            }
        })
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [{"type": "text", "text": {"content": formatted_date}}]
            }
        })
        blocks.append({"object": "block", "type": "divider", "divider": {}})
        
        # --- åƒèˆ‡è€…å€å¡Š ---
        participants = list(set(speaker_map.values()))
        if participants:
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "ğŸ‘¥ åƒèˆ‡è€…"}}]
                }
            })
            participant_text = ", ".join(participants)
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": participant_text}}]
                }
            })
            blocks.append({"object": "block", "type": "divider", "divider": {}})

        # --- æ‘˜è¦å€å¡Š ---
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "ğŸ“ æ‘˜è¦"}}]
            }
        })
        blocks.append({
            "object": "block",
            "type": "callout",
            "callout": {
                "rich_text": [{"type": "text", "text": {"content": summary}}],
                "icon": {"emoji": "ğŸ’¡"}
            }
        })
        blocks.append({"object": "block", "type": "divider", "divider": {}})

        # --- å¾…è¾¦äº‹é …å€å¡Š ---
        if todos:
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "âœ… å¾…è¾¦äº‹é …"}}]
                }
            })
            
            # æ·»åŠ å¾…è¾¦äº‹é …
            for todo in todos:
                blocks.append({
                    "object": "block",
                    "type": "to_do",
                    "to_do": {
                        "rich_text": [{"type": "text", "text": {"content": todo}}],
                        "checked": False
                    }
                })
            
            blocks.append({"object": "block", "type": "divider", "divider": {}})

        # --- æº–å‚™å®Œæ•´é€å­—ç¨¿ ---
        full_transcript = ""
        for segment in segments:
            speaker = segment["speaker"]
            text = segment["text"]
            timestamp = segment.get("timestamp", "")
            line = f"{timestamp} {speaker}: {text}" if timestamp else f"{speaker}: {text}"
            full_transcript += f"{line}\n"

        # --- ç”Ÿæˆè©³ç´°ç­†è¨˜ ---
        comprehensive_notes = self.generate_comprehensive_notes(full_transcript)
        
        # --- æ·»åŠ è©³ç´°ç­†è¨˜å€å¡Š ---
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "ğŸ“Š è©³ç´°ç­†è¨˜"}}]
            }
        })
        
        # ä½¿ç”¨ Notion æ ¼å¼åŒ–å™¨è™•ç†ç­†è¨˜
        note_blocks = self.notion_formatter.process_note_format_for_notion(comprehensive_notes)
        
        # API é™åˆ¶ï¼šæ¯æ¬¡è«‹æ±‚æœ€å¤š 100 å€‹å€å¡Š
        MAX_BLOCKS_PER_REQUEST = 90
        
        # æª¢æŸ¥å€å¡Šæ•¸é‡
        base_blocks_count = len(blocks)
        available_slots = MAX_BLOCKS_PER_REQUEST - base_blocks_count
        
        if len(note_blocks) > available_slots:
            blocks.extend(note_blocks[:available_slots])
            remaining_note_blocks = note_blocks[available_slots:]
        else:
            blocks.extend(note_blocks)
            remaining_note_blocks = []
        
        # æ·»åŠ åˆ†éš”ç·š
        remaining_note_blocks.append({"object": "block", "type": "divider", "divider": {}})

        # --- æ·»åŠ éŸ³æª”é€£çµ (å¦‚æœæœ‰) ---
        if file_id and google_service:
            try:
                file_details = google_service.get_file_details(file_id)
                file_name = file_details.get("name", "éŸ³é »æª”æ¡ˆ")
                file_link = file_details.get("webViewLink", f"https://drive.google.com/file/d/{file_id}")
                
                remaining_note_blocks.append({
                    "object": "block",
                    "type": "heading_2",
                    "heading_2": {
                        "rich_text": [{"type": "text", "text": {"content": "ğŸµ åŸå§‹éŒ„éŸ³"}}]
                    }
                })
                
                remaining_note_blocks.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [
                            {"type": "text", "text": {"content": "ğŸ“ éŒ„éŸ³æª”æ¡ˆ: "}},
                            {"type": "text", "text": {"content": file_name, "link": {"url": file_link}}}
                        ]
                    }
                })
                
                remaining_note_blocks.append({"object": "block", "type": "divider", "divider": {}})
            except Exception as e:
                logging.error(f"âŒ ç²å–æª”æ¡ˆé€£çµå¤±æ•—: {str(e)}")
        
        # --- å®Œæ•´é€å­—ç¨¿å€å¡Š ---
        remaining_note_blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "ğŸ™ï¸ å®Œæ•´é€å­—ç¨¿"}}]
            }
        })
        
        # ä½¿ç”¨ toggle å€å¡ŠåŒ…è£é€å­—ç¨¿
        transcript_blocks = []
        
        # æ·»åŠ å¸¶æ™‚é–“æˆ³çš„é€å­—ç¨¿
        for segment in segments:
            speaker = segment["speaker"]
            text = segment["text"]
            timestamp = segment.get("timestamp", "")
            
            # å¦‚æœæœ‰æ™‚é–“æˆ³ï¼ŒåŠ å…¥æ™‚é–“æˆ³
            content = f"{timestamp} **{speaker}**: {text}" if timestamp else f"**{speaker}**: {text}"
            
            transcript_blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": content}}]
                }
            })
        
        # åˆ†æ‰¹æ·»åŠ é€å­—ç¨¿ï¼Œä½¿ç”¨ toggle å€å¡Š
        MAX_TOGGLE_CHILDREN = 90
        
        # åˆ†å‰² transcript_blocks ç‚ºå¤šå€‹ toggle å€å¡Š
        for i in range(0, len(transcript_blocks), MAX_TOGGLE_CHILDREN):
            toggle_children = []
            end_idx = min(i + MAX_TOGGLE_CHILDREN, len(transcript_blocks))
            
            # åªåœ¨ç¬¬ä¸€å€‹ toggle å€å¡Šæ·»åŠ èªªæ˜
            if i == 0:
                toggle_children.append({
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{"type": "text", "text": {"content": "æ­¤å€å¡ŠåŒ…å«å®Œæ•´é€å­—ç¨¿"}}]
                    }
                })
                toggle_children.append({"object": "block", "type": "divider", "divider": {}})
            
            # æ·»åŠ æœ¬æ‰¹æ¬¡çš„ transcript_blocks
            toggle_children.extend(transcript_blocks[i:end_idx])
            
            # å»ºç«‹ toggle å€å¡Š
            toggle_title = "é»æ“Šå±•é–‹å®Œæ•´é€å­—ç¨¿"
            if len(transcript_blocks) > MAX_TOGGLE_CHILDREN:
                part_num = (i // MAX_TOGGLE_CHILDREN) + 1
                total_parts = (len(transcript_blocks) + MAX_TOGGLE_CHILDREN - 1) // MAX_TOGGLE_CHILDREN
                toggle_title = f"é»æ“Šå±•é–‹å®Œæ•´é€å­—ç¨¿ (ç¬¬ {part_num}/{total_parts} éƒ¨åˆ†)"
            
            remaining_note_blocks.append({
                "object": "block",
                "type": "toggle",
                "toggle": {
                    "rich_text": [{"type": "text", "text": {"content": toggle_title}}],
                    "children": toggle_children
                }
            })
        
        # Notion API è«‹æ±‚è¨­å®š
        headers = {
            "Authorization": f"Bearer {self.notion_token}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28"
        }

        try:
            # å»ºç«‹ä¸»é é¢
            data = {
                "parent": {"database_id": self.notion_database_id},
                "properties": {
                    "title": {
                        "title": [{"text": {"content": page_title}}]
                    }
                },
                "children": blocks
            }
            
            logging.info(f"- å»ºç«‹ Notion é é¢ (åŒ…å« {len(blocks)} å€‹å€å¡Š)")
            response = requests.post(
                "https://api.notion.com/v1/pages",
                headers=headers,
                json=data,
                timeout=30
            )
            response.raise_for_status()
            result = response.json()
            page_id = result["id"]
            page_url = result.get("url", f"https://www.notion.so/{page_id.replace('-', '')}")
            
            # è‹¥æœ‰å‰©é¤˜å€å¡Šï¼Œåˆ†æ‰¹æ·»åŠ 
            if remaining_note_blocks:
                total_batches = (len(remaining_note_blocks) + MAX_BLOCKS_PER_REQUEST - 1) // MAX_BLOCKS_PER_REQUEST
                logging.info(f"- åˆ†æ‰¹æ·»åŠ å‰©é¤˜å…§å®¹ (å…± {len(remaining_note_blocks)} å€‹å€å¡Šï¼Œåˆ† {total_batches} æ‰¹)")
                
                # æ·»åŠ å‰©é¤˜å€å¡Š
                for i in range(0, len(remaining_note_blocks), MAX_BLOCKS_PER_REQUEST):
                    end_idx = min(i + MAX_BLOCKS_PER_REQUEST, len(remaining_note_blocks))
                    batch_num = i // MAX_BLOCKS_PER_REQUEST + 1
                    
                    # æ·»åŠ é‡è©¦é‚è¼¯
                    max_retries = 3
                    for retry in range(max_retries):
                        try:
                            logging.info(f"- æ·»åŠ ç¬¬ {batch_num}/{total_batches} æ‰¹ (å˜—è©¦ {retry+1}/{max_retries})")
                            
                            batch_response = requests.patch(
                                f"https://api.notion.com/v1/blocks/{page_id}/children",
                                headers=headers,
                                json={"children": remaining_note_blocks[i:end_idx]},
                                timeout=30
                            )
                            batch_response.raise_for_status()
                            logging.info(f"âœ… ç¬¬ {batch_num}/{total_batches} æ‰¹æ·»åŠ æˆåŠŸ")
                            break
                            
                        except Exception as e:
                            if retry < max_retries - 1:
                                wait_time = 2 ** retry  # æŒ‡æ•¸é€€é¿
                                logging.warning(f"âš ï¸ ç¬¬ {batch_num} æ‰¹æ·»åŠ å¤±æ•—ï¼Œ{wait_time} ç§’å¾Œé‡è©¦: {str(e)}")
                                time.sleep(wait_time)
                            else:
                                logging.error(f"âŒ ç¬¬ {batch_num} æ‰¹æ·»åŠ å¤±æ•—: {str(e)}")
                    
                    # æ‰¹æ¬¡ä¹‹é–“çŸ­æš«æš«åœï¼Œé¿å… rate limit
                    if i + MAX_BLOCKS_PER_REQUEST < len(remaining_note_blocks):
                        time.sleep(1)
            
            logging.info(f"âœ… Notion é é¢å»ºç«‹å®Œæˆ (URL: {page_url})")
            return page_id, page_url
            
        except requests.exceptions.RequestException as e:
            # è©³ç´°è¨˜éŒ„ API éŒ¯èª¤
            error_message = f"Notion API è«‹æ±‚å¤±æ•—: {str(e)}"
            if hasattr(e, "response") and e.response is not None:
                status_code = e.response.status_code
                try:
                    error_data = e.response.json()
                    error_message = f"Notion API éŒ¯èª¤ ({status_code}): {error_data}"
                except:
                    error_message = f"Notion API éŒ¯èª¤ ({status_code}): {e.response.text}"
            
            logging.error(f"âŒ {error_message}")
            raise ValueError(error_message)
            
        except Exception as e:
            logging.error(f"âŒ å»ºç«‹ Notion é é¢æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {str(e)}")
            raise
    
    def extract_date_from_filename(self, filename: str) -> Optional[str]:
        """
        å¾æª”æ¡ˆåç¨±ä¸­æå–æ—¥æœŸ
        
        Args:
            filename: æª”æ¡ˆåç¨±
        
        Returns:
            æ—¥æœŸå­—ä¸² (YYYY-MM-DD) æˆ– None
        """
        # å˜—è©¦åŒ¹é… REC_YYYYMMDD_HHMMSS æ ¼å¼
        pattern1 = r'REC_(\d{8})_\d+'
        match1 = re.search(pattern1, filename)
        if match1:
            date_str = match1.group(1)
            try:
                date_obj = datetime.strptime(date_str, '%Y%m%d')
                return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                pass
        
        # å˜—è©¦åŒ¹é… [YYYY-MM-DD] æ ¼å¼
        pattern2 = r'\[(\d{4}-\d{2}-\d{2})\]'
        match2 = re.search(pattern2, filename)
        if match2:
            return match2.group(1)
        
        # å˜—è©¦åŒ¹é… YYYY-MM-DD æ ¼å¼
        pattern3 = r'(\d{4}-\d{2}-\d{2})'
        match3 = re.search(pattern3, filename)
        if match3:
            return match3.group(1)
        
        # å˜—è©¦åŒ¹é… YYYY/MM/DD æ ¼å¼
        pattern4 = r'(\d{4})[/\-](\d{2})[/\-](\d{2})'
        match4 = re.search(pattern4, filename)
        if match4:
            y, m, d = match4.groups()
            return f"{y}-{m}-{d}"
        
        return None
