import os
import sys
import tempfile
import shutil
import subprocess
import io
import json
import re
import time
import logging
import uuid
import threading
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from concurrent.futures import ThreadPoolExecutor
import requests
import atexit

# Google API ç›¸é—œ
from google.oauth2.credentials import Credentials
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# èªéŸ³è™•ç†ç›¸é—œ
import whisper
from pyannote.audio import Pipeline
import numpy as np
import soundfile as sf
import librosa

# LLM API ç›¸é—œ
import google.generativeai as genai

# æ·»åŠ å°å…¥ NotionFormatter
from ..utils.notion_formatter import NotionFormatter

# PDF è™•ç† (éœ€è¦ pip install PyPDF2)
try:
    import PyPDF2
except ImportError:
    print("âš ï¸ PyPDF2 æœªå®‰è£ï¼Œç„¡æ³•è™•ç† PDF é™„ä»¶ã€‚è«‹åŸ·è¡Œ 'pip install PyPDF2'")
    PyPDF2 = None

# å°å…¥å·¥ä½œç‹€æ…‹å¸¸æ•¸
from app.utils.constants import JOB_STATUS


class AudioProcessor:
    def __init__(self, max_workers=3):
        self.whisper_model = None
        self.diarization_pipeline = None
        self.drive_service = None
        self.oauth_drive_service = None  # å°ˆé–€ç”¨æ–¼OAuthèªè­‰çš„Driveæœå‹™
        
        # åˆå§‹åŒ–åŸ·è¡Œç·’æ± 
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        # è¨»å†Š executor é—œé–‰å‡½æ•¸
        atexit.register(self.shutdown_executor)
        # å·¥ä½œç‹€æ…‹è¿½è¹¤
        self.jobs = {}
        # ç¢ºä¿ç·šç¨‹å®‰å…¨çš„é–
        self.jobs_lock = threading.Lock()
        # åˆå§‹åŒ– Notion æ ¼å¼åŒ–å·¥å…·
        self.notion_formatter = NotionFormatter()
        # åˆå§‹åŒ–æœå‹™
        self.init_services()

    def init_services(self):
        """åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„æœå‹™"""
        logging.info("ğŸ”„ åˆå§‹åŒ–æœå‹™ä¸­...")
        
        # ç²å–æœå‹™å¸³è™Ÿè·¯å¾‘
        sa_json_path = os.getenv("GOOGLE_SA_JSON_PATH", "credentials/service-account.json")
        client_secret_path = os.getenv("GOOGLE_CLIENT_SECRET_PATH", "credentials/client_secret.json")
        
        # åˆå§‹åŒ–æœå‹™å¸³è™Ÿèªè­‰çš„Drive API (ç”¨æ–¼ä¸‹è¼‰æª”æ¡ˆ)
        try:
            # æª¢æŸ¥æœå‹™å¸³è™Ÿæ–‡ä»¶è·¯å¾‘
            if not os.path.isabs(sa_json_path):
                # å¦‚æœæ˜¯ç›¸å°è·¯å¾‘ï¼Œå…ˆå˜—è©¦ç›¸å°æ–¼ç•¶å‰å·¥ä½œç›®éŒ„
                if os.path.exists(sa_json_path):
                    sa_json_path = os.path.abspath(sa_json_path)
                # å†å˜—è©¦ç›¸å°æ–¼æ‡‰ç”¨æ ¹ç›®éŒ„
                elif os.path.exists(os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), sa_json_path)):
                    sa_json_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), sa_json_path)
                
            # ç¢ºèªæœå‹™å¸³è™Ÿæ–‡ä»¶å­˜åœ¨
            if not os.path.exists(sa_json_path):
                logging.error(f"âŒ æ‰¾ä¸åˆ°æœå‹™å¸³è™Ÿæ–‡ä»¶: {sa_json_path}")
                # å˜—è©¦ä½¿ç”¨é è¨­è·¯å¾‘
                alternative_paths = [
                    "/app/credentials/service-account.json",
                    "./credentials/service-account.json",
                    os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "credentials/service-account.json")
                ]
                
                for path in alternative_paths:
                    if os.path.exists(path):
                        logging.info(f"âœ… æ‰¾åˆ°æ›¿ä»£æœå‹™å¸³è™Ÿè·¯å¾‘: {path}")
                        sa_json_path = path
                        break
                else:
                    raise FileNotFoundError(f"ç„¡æ³•æ‰¾åˆ°æœå‹™å¸³è™ŸJSONæª”æ¡ˆï¼Œå·²å˜—è©¦çš„è·¯å¾‘: {sa_json_path} å’Œ {alternative_paths}")
                
            # ä½¿ç”¨æœå‹™å¸³è™Ÿ
            logging.info(f"ğŸ”„ æ­£åœ¨ä½¿ç”¨æœå‹™å¸³è™Ÿæ–‡ä»¶: {sa_json_path}")
            service_credentials = service_account.Credentials.from_service_account_file(
                sa_json_path,
                scopes=['https://www.googleapis.com/auth/drive']
            )
            self.drive_service = build('drive', 'v3', credentials=service_credentials)
            logging.info("âœ… ä½¿ç”¨æœå‹™å¸³è™Ÿåˆå§‹åŒ– Drive API æˆåŠŸ")
        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ–æœå‹™å¸³è™Ÿ Google Drive API å¤±æ•—: {str(e)}")
            self.drive_service = None
        
        # åˆå§‹åŒ– Google Gemini API
        try:
            gemini_api_key = os.getenv("GEMINI_API_KEY")
            if not gemini_api_key:
                logging.warning("âš ï¸ æœªè¨­ç½®GEMINI_API_KEYç’°å¢ƒè®Šé‡ï¼ŒæŸäº›åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
            else:
                genai.configure(api_key=gemini_api_key)
                logging.info("âœ… åˆå§‹åŒ– Gemini API æˆåŠŸ")
        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ– Gemini API å¤±æ•—: {str(e)}")
        
        logging.info("âœ… æœå‹™åˆå§‹åŒ–å®Œæˆ")
    
    def set_oauth_credentials(self, credentials):
        """è¨­ç½®OAuthæ†‘è­‰ä¸¦åˆå§‹åŒ–Driveæœå‹™ï¼Œä½†ä¸å¯«å…¥æ–‡ä»¶"""
        try:
            # ç›´æ¥ä½¿ç”¨æä¾›çš„æ†‘è­‰ä¾†å»ºç«‹oauth_drive_service
            # ä¸å†å¯«å…¥æ†‘è­‰åˆ°æ–‡ä»¶ç³»çµ±
            self.oauth_drive_service = build('drive', 'v3', credentials=credentials)
            logging.info("âœ… ä½¿ç”¨OAuthæ†‘è­‰åˆå§‹åŒ–Drive APIæˆåŠŸ")
            
            # è¨˜éŒ„æ†‘è­‰çš„æœ‰æ•ˆæœŸé™
            if hasattr(credentials, 'expiry'):
                expiry_time = credentials.expiry.strftime('%Y-%m-%d %H:%M:%S') if credentials.expiry else "æœªçŸ¥"
                logging.info(f"ğŸ“ OAuthæ†‘è­‰æœ‰æ•ˆæœŸè‡³: {expiry_time}")
                
            return True
        except Exception as e:
            logging.error(f"âŒ ä½¿ç”¨OAuthæ†‘è­‰åˆå§‹åŒ–Drive APIå¤±æ•—: {str(e)}")
            self.oauth_drive_service = None
            return False

    def download_file(self, file_id: str, target_dir: str) -> str: # Returns filename
        """å¾ Google Drive ä¸‹è¼‰æª”æ¡ˆåˆ°æŒ‡å®šçš„ç›®æ¨™ç›®éŒ„ (ä½¿ç”¨æœå‹™å¸³è™Ÿ)"""
        logging.info(f"ğŸ”„ å¾ Google Drive ä¸‹è¼‰æª”æ¡ˆ (ID: {file_id}) åˆ°ç›®éŒ„ {target_dir}")
        
        try:
            if not self.drive_service:
                raise RuntimeError("æœå‹™å¸³è™Ÿ Drive API æœªåˆå§‹åŒ–ï¼Œç„¡æ³•ä¸‹è¼‰æª”æ¡ˆ")
            
            # Ensure target_dir exists
            os.makedirs(target_dir, exist_ok=True)
            
            file_meta = self.drive_service.files().get(
                fileId=file_id, fields="name,mimeType"
            ).execute()
            
            raw_file_name = file_meta.get('name', f"file_{file_id}")
            safe_file_name = re.sub(r'[\\/*?:"<>|]', "_", raw_file_name)
            local_path = os.path.join(target_dir, safe_file_name)
            
            request_obj = self.drive_service.files().get_media(fileId=file_id)
            
            with open(local_path, 'wb') as f:
                downloader = MediaIoBaseDownload(f, request_obj)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
                    # logging.debug(f"ä¸‹è¼‰é€²åº¦: {int(status.progress() * 100)}%") # Can be verbose
            
            logging.info(f"âœ… æª”æ¡ˆä¸‹è¼‰å®Œæˆ: {safe_file_name} (å„²å­˜æ–¼ {target_dir})")
            return safe_file_name # Return just the filename
            
        except Exception as e:
            logging.error(f"âŒ ä¸‹è¼‰æª”æ¡ˆ ID {file_id} åˆ° {target_dir} å¤±æ•—: {str(e)}")
            raise

    def download_from_drive(self, file_id: str) -> Tuple[str, str]:
        """å¾ Google Drive ä¸‹è¼‰æª”æ¡ˆåˆ°è‡¨æ™‚ç›®éŒ„ (ä½¿ç”¨æœå‹™å¸³è™Ÿ)"""
        logging.info(f"ğŸ”„ å¾ Google Drive ä¸‹è¼‰æª”æ¡ˆ (ID: {file_id})")
        
        try:
            # ç¢ºä¿æœå‹™å¸³è™Ÿå·²ç¶“åˆå§‹åŒ–
            if not self.drive_service:
                raise RuntimeError("æœå‹™å¸³è™Ÿ Drive API æœªåˆå§‹åŒ–ï¼Œç„¡æ³•ä¸‹è¼‰æª”æ¡ˆ")
            
            # å»ºç«‹è‡¨æ™‚ç›®éŒ„
            temp_dir = tempfile.mkdtemp()
            
            # ç²å–æ–‡ä»¶è³‡è¨Š
            file_meta = self.drive_service.files().get(
                fileId=file_id, fields="name,mimeType"
            ).execute()
            
            # ç²å–æª”æ¡ˆåç¨±ä¸¦æ¸…ç†ä¸å®‰å…¨çš„å­—å…ƒ
            raw_file_name = file_meta.get('name', f"file_{file_id}")
            # ç§»é™¤æ–œç·šç­‰ä¸å®‰å…¨å­—å…ƒï¼Œé¿å…è·¯å¾‘å•é¡Œ
            safe_file_name = re.sub(r'[\\/*?:"<>|]', "_", raw_file_name)
            local_path = os.path.join(temp_dir, safe_file_name)
            
            # ä¸‹è¼‰æª”æ¡ˆ
            request = self.drive_service.files().get_media(fileId=file_id)
            
            with open(local_path, 'wb') as f:
                downloader = MediaIoBaseDownload(f, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
                    logging.debug(f"ä¸‹è¼‰é€²åº¦: {int(status.progress() * 100)}%")
            
            logging.info(f"âœ… æª”æ¡ˆä¸‹è¼‰å®Œæˆ: {safe_file_name} (å„²å­˜æ–¼ {temp_dir})")
            return local_path, temp_dir
            
        except Exception as e:
            logging.error(f"âŒ ä¸‹è¼‰æª”æ¡ˆå¤±æ•—: {str(e)}")
            if 'temp_dir' in locals() and os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
            raise
    
    def list_drive_files(self, query="trashed = false and (mimeType contains 'audio/' or mimeType = 'application/pdf')"):
        """åˆ—å‡ºGoogle Driveæª”æ¡ˆ (ä½¿ç”¨OAuthèªè­‰)"""
        logging.info(f"ğŸ”„ ä½¿ç”¨OAuthæ†‘è­‰åˆ—å‡ºGoogle Driveæª”æ¡ˆ")

        if not self.oauth_drive_service:
            logging.error("âŒ OAuth Driveæœå‹™æœªåˆå§‹åŒ–ï¼Œç„¡æ³•å–å¾—æª”æ¡ˆåˆ—è¡¨")
            return []

        try:
            # åŸ·è¡ŒæŸ¥è©¢ï¼Œå–å¾—æª”æ¡ˆ IDã€åç¨±ã€MIME é¡å‹ã€å¤§å°èˆ‡çˆ¶è³‡æ–™å¤¾
            results = self.oauth_drive_service.files().list(
                q=query,
                spaces='drive',
                fields="files(id, name, mimeType, size, parents)",
                orderBy="modifiedTime desc"
            ).execute()

            files = results.get('files', [])
            logging.info(f"âœ… å·²æˆåŠŸç²å– {len(files)} å€‹æª”æ¡ˆ")
            return files
        except Exception as e:
            logging.error(f"âŒ åˆ—å‡ºGoogle Driveæª”æ¡ˆå¤±æ•—: {str(e)}")
            return []

    def find_folder_id_by_path(self, folder_path: str) -> Optional[str]:
        """
        æ ¹æ“šå¤šå±¤è³‡æ–™å¤¾åç¨±ï¼ˆå¦‚ 'WearNote_Recordings/Documents'ï¼‰éè¿´æŸ¥æ‰¾æœ€çµ‚è³‡æ–™å¤¾IDã€‚
        """
        if not self.oauth_drive_service:
            return None
        names = folder_path.strip('/').split('/')
        parent_id = 'root'
        for name in names:
            results = self.oauth_drive_service.files().list(
                q=f"trashed = false and mimeType = 'application/vnd.google-apps.folder' and name = '{name}' and '{parent_id}' in parents",
                spaces='drive',
                fields="files(id, name)",
                pageSize=10
            ).execute()
            folders = results.get('files', [])
            if not folders:
                return None
            parent_id = folders[0]['id']
        return parent_id

    def download_and_extract_text(self, file_id: str) -> Tuple[Optional[str], Optional[str]]:
        """ä¸‹è¼‰ä¸¦æå– PDF æ–‡å­—å…§å®¹ (ä½¿ç”¨æœå‹™å¸³è™Ÿ)"""
        try:
            # ç²å–æ–‡ä»¶è³‡è¨Š
            file_meta = self.drive_service.files().get(
                fileId=file_id, fields="name,mimeType"
            ).execute()
            
            mime_type = file_meta.get('mimeType', '')
            
            # ç›®å‰åƒ…æ”¯æŒ PDF
            if mime_type != 'application/pdf' or PyPDF2 is None:
                return None, None
            
            # ä¸‹è¼‰æ–‡ä»¶
            local_path, temp_dir = self.download_from_drive(file_id)
            
            # æå– PDF æ–‡å­—
            text = ""
            with open(local_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    text += page.extract_text()
            
            return text, temp_dir
        except Exception as e:
            logging.error(f"âŒ æå–PDFæ–‡å­—å¤±æ•—: {str(e)}")
            if 'temp_dir' in locals() and temp_dir:
                return None, temp_dir
            return None, None

    def preprocess_audio(self, audio_path: str) -> str:
        """é è™•ç†éŸ³é » (åƒ…ç¢ºä¿æ ¼å¼æ­£ç¢º)"""
        logging.info(f"ğŸ”„ é è™•ç†éŸ³é »: {os.path.basename(audio_path)}")
        
        # ç¢ºä¿æª”æ¡ˆç‚º WAV æ ¼å¼
        if not audio_path.lower().endswith('.wav'):
            audio_path = self.convert_to_wav(audio_path)
        
        logging.info(f"âœ… éŸ³é »é è™•ç†å®Œæˆ")
        return audio_path

    def rename_drive_file(self, file_id: str, new_name: str) -> bool:
        """æ ¹æ“šè™•ç†çµæœé‡å‘½å Google Drive ä¸Šçš„æª”æ¡ˆ (ä½¿ç”¨æœå‹™å¸³è™Ÿ)"""
        try:
            if not self.drive_service:
                raise RuntimeError("æœå‹™å¸³è™Ÿ Drive API æœªåˆå§‹åŒ–ï¼Œç„¡æ³•é‡å‘½åæª”æ¡ˆ")
                
            self.drive_service.files().update(
                fileId=file_id,
                body={'name': new_name}
            ).execute()
            logging.info(f"âœ… æˆåŠŸé‡å‘½å Google Drive æª”æ¡ˆ: {new_name}")
            return True
        except Exception as e:
            logging.error(f"âŒ é‡å‘½å Google Drive æª”æ¡ˆå¤±æ•—: {str(e)}")
            return False
        
    def format_timestamp(self, seconds: float) -> str:
            """å°‡ç§’æ•¸è½‰æ›ç‚ºå¯è®€æ™‚é–“æˆ³è¨˜"""
            minutes, seconds = divmod(int(seconds), 60)
            hours, minutes = divmod(minutes, 60)
            
            if hours > 0:
                return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
            else:
                return f"{minutes:02d}:{seconds:02d}"
            
    def extract_date_from_filename(self, filename: str) -> Optional[str]:
        """å¾æª”æ¡ˆåç¨±ä¸­æå–æ—¥æœŸï¼Œæ”¯æ´å¤šç¨®æ ¼å¼"""
        # å˜—è©¦åŒ¹é… REC_YYYYMMDD_HHMMSS æ ¼å¼
        pattern1 = r'REC_(\d{8})_\d+'
        match1 = re.search(pattern1, filename)
        if match1:
            date_str = match1.group(1)
            try:
                # å°‡ YYYYMMDD è½‰æ›ç‚º YYYY-MM-DD
                date_obj = datetime.strptime(date_str, '%Y%m%d')
                return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                pass
        
        # å˜—è©¦åŒ¹é…å·²æœ‰çš„ [YYYY-MM-DD] æ ¼å¼
        pattern2 = r'\[(\d{4}-\d{2}-\d{2})\]'
        match2 = re.search(pattern2, filename)
        if match2:
            return match2.group(1)
            
        # å˜—è©¦åŒ¹é…å…¶ä»–å¯èƒ½çš„æ—¥æœŸæ ¼å¼ (YYYY-MM-DD)
        pattern3 = r'(\d{4}-\d{2}-\d{2})'
        match3 = re.search(pattern3, filename)
        if match3:
            return match3.group(1)
            
        # å¦‚æœéƒ½ç„¡æ³•åŒ¹é…ï¼Œè¿”å› None
        return None

    def get_file_folder_path(self, file_id):
        """ç²å–æª”æ¡ˆæ‰€åœ¨çš„å®Œæ•´è³‡æ–™å¤¾è·¯å¾‘"""
        try:
            if not self.oauth_drive_service:
                logging.error("æœªåˆå§‹åŒ– Drive æœå‹™ï¼Œç„¡æ³•ç²å–è³‡æ–™å¤¾è·¯å¾‘")
                return ""
            
            # ç²å–æª”æ¡ˆå…ƒæ•¸æ“šä»¥æ‰¾åˆ°çˆ¶è³‡æ–™å¤¾ID
            file = self.oauth_drive_service.files().get(
                fileId=file_id, 
                fields="parents"
            ).execute()
            
            if not file.get('parents'):
                return "root"
            
            # æ§‹å»ºè³‡æ–™å¤¾è·¯å¾‘
            path = []
            parent_id = file['parents'][0]
            
            # å‘ä¸Šå°‹æ‰¾çˆ¶è³‡æ–™å¤¾ç›´åˆ°æ ¹ç›®éŒ„
            max_depth = 10  # é˜²æ­¢ç„¡é™å¾ªç’°
            depth = 0
            
            while parent_id and depth < max_depth:
                try:
                    parent = self.oauth_drive_service.files().get(
                        fileId=parent_id,
                        fields="id,name,parents"
                    ).execute()
                    
                    path.insert(0, parent.get('name', 'unknown'))
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰çˆ¶è³‡æ–™å¤¾
                    if 'parents' in parent and parent['parents']:
                        parent_id = parent['parents'][0]
                    else:
                        break
                        
                    depth += 1
                    
                except Exception as e:
                    logging.error(f"ç²å–çˆ¶è³‡æ–™å¤¾è³‡è¨Šå¤±æ•—: {e}")
                    break
            
            # è¿”å›ç”± / é€£æ¥çš„è³‡æ–™å¤¾è·¯å¾‘
            return "/".join(path)
            
        except Exception as e:
            logging.error(f"ç²å–æª”æ¡ˆè³‡æ–™å¤¾è·¯å¾‘å¤±æ•—: {e}")
            return ""

    def try_multiple_gemini_models(self, system_prompt: str, user_content: str, 
                                models: List[str] = None) -> Any:
        """Try generating content using multiple Gemini models until one succeeds.
        
        Args:
            system_prompt: The system instructions for the model
            user_content: The user content to process
            models: List of model names to try in order (uses default list if None)
            
        Returns:
            The successful generation response
            
        Raises:
            Exception: If all models fail
        """
        # Default models list if none provided
        if models is None:
            models = ['gemini-2.5-pro-exp-03-25', 'gemini-2.5-flash-preview-04-17',
                        'gemini-1.5-pro', 'gemini-2.0-flash', 'gemini-1.5-flash', 'gemini-2.0-flash-lite']
        
        response = None
        last_error = None

        # Try different models until successful
        for model_name in models:
            try:
                logging.info(f"ğŸ”„ ä½¿ç”¨æ¨¡å‹: {model_name}")
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(
                    [system_prompt, user_content]
                )
                # If successful, break the loop
                logging.info(f"âœ… æˆåŠŸä½¿ç”¨æ¨¡å‹ {model_name} ç”Ÿæˆç­†è¨˜")
                break
            except Exception as e:
                last_error = e
                # Check if quota error
                if "429" in str(e) or "quota" in str(e).lower():
                    # Extract and log the quota documentation URL
                    url_match = re.search(r'https?://\S+', str(e))
                    logging.warning(f"âš ï¸ æ¨¡å‹ {model_name} é…é¡å·²ç”¨ç›¡: {url_match.group(0)}")
                    # Continue to next model
                    continue
                else:
                    # Raise other errors
                    logging.error(f"âŒ ä½¿ç”¨æ¨¡å‹ {model_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    raise

        # Check if all models failed
        if response is None:
            logging.error("âŒ æ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—äº†")
            raise last_error
        
        return response

    def generate_comprehensive_notes(self, transcript: str) -> str:
        """ä½¿ç”¨ Gemini API ç”Ÿæˆçµæ§‹åŒ–çš„ç­†è¨˜"""
        logging.info("ğŸ”„ ç”Ÿæˆç­†è¨˜...")
        
        try:
            # ä½¿ç”¨æ›´è©³ç´°çš„Markdownæ ¼å¼æŒ‡ç¤º
            system_prompt = """
            ä½ å…·å‚™é›»å­å·¥ç¨‹é€šè¨Šç›¸é—œèƒŒæ™¯ï¼Œèƒ½å¤ ç†è§£æŠ€è¡“æ€§å…§å®¹(åŒ…æ‹¬ä¸€äº›å¸¸è½åˆ°çš„socket, RIC, gNB, nFAPI, OAIç­‰è¡“èª)ã€‚
            å°‡éŒ„éŸ³é€å­—ç¨¿æ•´ç†æˆç­†è¨˜å…§å®¹ï¼Œè«‹ä½¿ç”¨Markdownæ ¼å¼ç›´æ¥è¼¸å‡ºç­†è¨˜å…§å®¹:
            é¿å…ä½¿ç”¨```markdown```ï¼Œç›´æ¥è¼¸å‡ºMarkdownæ ¼å¼çš„ç­†è¨˜å…§å®¹ã€‚
            """

            # Use the function in generate_comprehensive_notes
            response = self.try_multiple_gemini_models(
                system_prompt,
                f"æœƒè­°é€å­—ç¨¿ï¼š\n{transcript}"
            )
            
            comprehensive_notes = response.text
            logging.info("âœ… ç­†è¨˜ç”ŸæˆæˆåŠŸ")
            return comprehensive_notes
            
        except Exception as e:
            logging.error(f"âŒ ç­†è¨˜ç”Ÿæˆå¤±æ•—: {str(e)}")
            return "ç­†è¨˜ç”Ÿæˆå¤±æ•—ï¼Œè«‹åƒè€ƒæœƒè­°æ‘˜è¦å’Œå®Œæ•´è¨˜éŒ„ã€‚"

    def create_notion_page(self, title: str, summary: str, todos: List[str], segments: List[Dict[str, Any]], speaker_map: Dict[str, str], file_id: str = None) -> Tuple[str, str]:
        """å»ºç«‹å–®ä¸€ Notion é é¢ï¼ŒåŒ…å«æ¨™é¡Œã€æ—¥æœŸã€åƒèˆ‡è€…ã€æ‘˜è¦ã€å¾…è¾¦äº‹é …ã€å®Œæ•´ç­†è¨˜èˆ‡å…§åµŒçš„é€å­—ç¨¿"""
        logging.info("ğŸ”„ å»ºç«‹ Notion é é¢...")

        notion_token = os.getenv("NOTION_TOKEN")
        database_id = os.getenv("NOTION_DATABASE_ID")

        if not notion_token or not database_id:
            raise ValueError("ç¼ºå°‘ Notion API è¨­å®š")

        # --- æº–å‚™é é¢å…§å®¹å€å¡Š ---
        blocks = []
        
        # å¾æª”æ¡ˆåç¨±æå–æ—¥æœŸæˆ–ä½¿ç”¨ç•¶å‰æ—¥æœŸ
        current_date = datetime.now()
        current_date_str = current_date.strftime("%Y-%m-%d")
        
        # å˜—è©¦å¾æª”æ¡ˆIDç²å–æª”æ¡ˆåç¨±ä¸¦æå–æ—¥æœŸ
        file_date = None
        if file_id:
            try:
                file_meta = self.drive_service.files().get(
                    fileId=file_id, fields="name"
                ).execute()
                filename = file_meta.get('name', '')
                if filename:
                    file_date = self.extract_date_from_filename(filename)
            except Exception as e:
                logging.error(f"âŒ ç²å–æª”æ¡ˆæ—¥æœŸå¤±æ•—: {str(e)}")
        
        # ä½¿ç”¨æª”æ¡ˆæ—¥æœŸæˆ–ç•¶å‰æ—¥æœŸ
        date_str = file_date if file_date else current_date_str
        formatted_date = datetime.strptime(date_str, "%Y-%m-%d").strftime("%Yå¹´%mæœˆ%dæ—¥")

        # --- æ¨™é¡Œå€å¡Š (ä½¿ç”¨æ—¥æœŸ+éŒ„éŸ³æª”æ¡ˆåç¨±) ---
        page_title = f"{formatted_date} {title}"

        # --- æ—¥æœŸå€å¡Š ---
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
            "rich_text": [{"type": "text", "text": {"content": "ğŸ“… æ—¥æœŸ"}}]
            }
        })
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
            "rich_text": [{"type": "text", "text": {"content": formatted_date}}]
            }
        })

        blocks.append({"object": "block", "type": "divider", "divider": {}})
        # --- åƒèˆ‡è€…å€å¡Š ---
        participants = list(set(speaker_map.values()))  # ç²å–å”¯ä¸€è­˜åˆ¥çš„åç¨±
        if participants:
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "ğŸ‘¥ åƒèˆ‡è€…"}}]
                }
            })
            participant_text = ", ".join(participants)
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": participant_text}}]
                }
            })
            blocks.append({"object": "block", "type": "divider", "divider": {}})

        # --- æ‘˜è¦å€å¡Š ---
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "ğŸ“ æ‘˜è¦"}}]
            }
        })
        blocks.append({
            "object": "block",
            "type": "callout",
            "callout": {
                "rich_text": [{"type": "text", "text": {"content": summary}}],
                "icon": {"emoji": "ğŸ’¡"}
            }
        })
        blocks.append({"object": "block", "type": "divider", "divider": {}})

        # --- å¾…è¾¦äº‹é …å€å¡Š ---
        if todos:
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "âœ… å¾…è¾¦äº‹é …"}}]
                }
            })
            
            # ä½¿ç”¨ todo list å‘ˆç¾å¾…è¾¦äº‹é …
            todo_blocks = []
            for todo in todos:
                todo_blocks.append({
                    "object": "block",
                    "type": "to_do",
                    "to_do": {
                        "rich_text": [{"type": "text", "text": {"content": todo}}],
                        "checked": False
                    }
                })
            
            blocks.extend(todo_blocks)
            blocks.append({"object": "block", "type": "divider", "divider": {}})

        # --- è™•ç†å®Œæ•´é€å­—ç¨¿ (ç„¡æ™‚é–“æˆ³è¨˜) ---
        full_transcript = ""
        
        for segment in segments:
            speaker = segment["speaker"]
            text = segment["text"]
            content = f"{speaker}: {text}"
            full_transcript += f"{content}\n"

        # --- ç”Ÿæˆå®Œæ•´ç­†è¨˜ ---
        comprehensive_notes = self.generate_comprehensive_notes(full_transcript)
        
        # --- å®Œæ•´ç­†è¨˜å€å¡Š ---
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "ğŸ“Š è©³ç´°ç­†è¨˜"}}]
            }
        })
        
        # Notion API é™åˆ¶ï¼šæ¯æ¬¡è«‹æ±‚æœ€å¤š 100 å€‹å€å¡Š
        MAX_BLOCKS_PER_REQUEST = 90  # ä½¿ç”¨ 90 ä½œç‚ºå®‰å…¨ç•Œé™
        
        # ä½¿ç”¨ NotionFormatter ä¾†è™•ç†ç­†è¨˜
        note_blocks = self.notion_formatter.process_note_format_for_notion(comprehensive_notes)
        
        # è¨ˆç®—å·²æœ‰å€å¡Šæ•¸
        base_blocks_count = len(blocks)
        available_slots = MAX_BLOCKS_PER_REQUEST - base_blocks_count
        
        # å¦‚æœç­†è¨˜å€å¡Šå¤ªå¤šï¼Œåƒ…æ·»åŠ èƒ½å®¹ç´çš„éƒ¨åˆ†
        if len(note_blocks) > available_slots:
            blocks.extend(note_blocks[:available_slots])
            remaining_note_blocks = note_blocks[available_slots:]
        else:
            blocks.extend(note_blocks)
            remaining_note_blocks = []
        
        headers = {
            "Authorization": f"Bearer {notion_token}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28"
        }

        try:
            # å»ºç«‹ä¸»é é¢ (åŒ…å«æ‰€æœ‰åŸºæœ¬ä¿¡æ¯)
            data = {
                "parent": {"database_id": database_id},
                "properties": {
                    "title": {
                        "title": [{"text": {"content": page_title}}]
                    }
                },
                "children": blocks
            }
            
            logging.info(f"- å»ºç«‹ Notion é é¢ (åŒ…å« {len(blocks)} å€‹å€å¡Šï¼Œé™åˆ¶ç‚º 100)")
            response = requests.post(
                "https://api.notion.com/v1/pages",
                headers=headers,
                json=data
            )
            response.raise_for_status()
            result = response.json()
            page_id = result["id"]
            page_url = result.get("url", f"https://www.notion.so/{page_id.replace('-', '')}")
            
            # å°‡é€å­—ç¨¿åˆ†æˆå¤šå€‹æ®µè½ (å› ç‚º Notion API æœ‰å­—ç¬¦é™åˆ¶)
            remaining_note_blocks.append({"object": "block", "type": "divider", "divider": {}})

            # --- å…§åµŒå®Œæ•´é€å­—ç¨¿å€å¡Š (ä½¿ç”¨ toggle å€å¡Š) ---
            remaining_note_blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "ğŸ™ï¸ å®Œæ•´é€å­—ç¨¿"}}]
                }
            })
            
            # ä½¿ç”¨ NotionFormatter çš„ split_transcript_into_blocks ç²å–é€å­—ç¨¿å€å¡Š
            transcript_blocks = self.notion_formatter.split_transcript_into_blocks(full_transcript)
            
            # å»ºç«‹éŸ³é »æª”æ¡ˆé€£çµå€å¡Š
            audio_link_blocks = []
            if file_id:
                try:
                    file_info = self.drive_service.files().get(
                        fileId=file_id, fields="name,webViewLink"
                    ).execute()
                    file_name = file_info.get('name', 'éŸ³é »æª”æ¡ˆ')
                    file_link = file_info.get('webViewLink', f"https://drive.google.com/file/d/{file_id}/view")
                    
                    audio_link_blocks.append({
                        "object": "block",
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": [
                                {"type": "text", "text": {"content": "ğŸ“ éŒ„éŸ³æª”æ¡ˆ: "}},
                                {"type": "text", "text": {"content": file_name, "link": {"url": file_link}}}
                            ]
                        }
                    })
                    audio_link_blocks.append({"object": "block", "type": "divider", "divider": {}})
                except Exception as e:
                    logging.error(f"âŒ ç²å–æª”æ¡ˆé€£çµå¤±æ•—: {str(e)}")
            
            # æ·»åŠ æª”æ¡ˆé€£çµåˆ° remaining_note_blocks
            remaining_note_blocks.extend(audio_link_blocks)
            
            # è¨ˆç®—æ¯å€‹ toggle å€å¡Šæœ€å¤šå¯ä»¥åŒ…å«çš„ transcript_blocks æ•¸é‡ (æœ€å¤§100å€‹)
            MAX_TOGGLE_CHILDREN = 90  # ä¿ç•™ä¸€äº›ç©ºé–“çµ¦å…¶ä»–å…ƒç´ 
            
            # åˆ†å‰² transcript_blocks ç‚ºå¤šå€‹ toggle å€å¡Š
            for i in range(0, len(transcript_blocks), MAX_TOGGLE_CHILDREN):
                toggle_children = []
                end_idx = min(i + MAX_TOGGLE_CHILDREN, len(transcript_blocks))
                
                # åªåœ¨ç¬¬ä¸€å€‹ toggle å€å¡Šæ·»åŠ èªªæ˜æ–‡å­—
                if i == 0:
                    toggle_children.append({
                        "object": "block",
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": [{"type": "text", "text": {"content": "æ­¤å€å¡ŠåŒ…å«å®Œæ•´é€å­—ç¨¿å…§å®¹"}}]
                        }
                    })
                    toggle_children.append({
                        "object": "block",
                        "type": "divider",
                        "divider": {}
                    })
                
                # æ·»åŠ æœ¬æ‰¹æ¬¡çš„ transcript_blocks
                toggle_children.extend(transcript_blocks[i:end_idx])
                
                # å»ºç«‹ç›®å‰æ‰¹æ¬¡çš„ toggle å€å¡Š
                toggle_title = "é»æ“Šå±•é–‹å®Œæ•´é€å­—ç¨¿"
                if i > 0:  # å¦‚æœä¸æ˜¯ç¬¬ä¸€å€‹ toggleï¼Œæ·»åŠ åºè™Ÿ
                    part_num = (i // MAX_TOGGLE_CHILDREN) + 1
                    total_parts = (len(transcript_blocks) + MAX_TOGGLE_CHILDREN - 1) // MAX_TOGGLE_CHILDREN
                    toggle_title = f"é»æ“Šå±•é–‹å®Œæ•´é€å­—ç¨¿ (ç¬¬ {part_num}/{total_parts} éƒ¨åˆ†)"
                
                remaining_note_blocks.append({
                    "object": "block",
                    "type": "toggle",
                    "toggle": {
                        "rich_text": [{"type": "text", "text": {"content": toggle_title}}],
                        "children": toggle_children
                    }
                })
            
            total_batches = (len(remaining_note_blocks) + MAX_BLOCKS_PER_REQUEST - 1) // MAX_BLOCKS_PER_REQUEST
            logging.info(f"- é–‹å§‹åˆ†æ‰¹æ·»åŠ é€å­—ç¨¿å…§å®¹ (å…± {len(remaining_note_blocks)} æ®µï¼Œåˆ† {total_batches} æ‰¹)")

            # Validate Notion token before making requests
            if not notion_token or notion_token.strip() == "":
                raise ValueError("ç„¡æ•ˆçš„ Notion API Token")

            # Add retry logic for batch additions
            for i in range(0, len(remaining_note_blocks), MAX_BLOCKS_PER_REQUEST):
                end_idx = min(i + MAX_BLOCKS_PER_REQUEST, len(remaining_note_blocks))
                batch_num = i // MAX_BLOCKS_PER_REQUEST + 1
                max_retries = 3
                retry_count = 0
                
                while retry_count < max_retries:
                    try:
                        logging.info(f"- æ·»åŠ ç¬¬ {batch_num}/{total_batches} æ‰¹é€å­—ç¨¿å…§å®¹ (ç¬¬ {retry_count + 1} æ¬¡å˜—è©¦)")
                        transcript_blocks_response = requests.patch(
                            f"https://api.notion.com/v1/blocks/{page_id}/children",
                            headers=headers,
                            json={"children": remaining_note_blocks[i:end_idx]},
                            timeout=30  # Add timeout to prevent hanging requests
                        )
                        
                        if transcript_blocks_response.status_code in [401, 403]:
                            logging.error(f"âŒ èªè­‰éŒ¯èª¤ (ç‹€æ…‹ç¢¼: {transcript_blocks_response.status_code}): è«‹ç¢ºèª Notion API Token æœ‰æ•ˆ")
                            try:
                                error_details = transcript_blocks_response.json()
                                logging.error(f"   è©³ç´°éŒ¯èª¤: {json.dumps(error_details, indent=2, ensure_ascii=False)}")
                            except:
                                logging.error(f"   å›æ‡‰å…§å®¹: {transcript_blocks_response.text}")
                            break  # Authentication errors won't be fixed by retrying
                            
                        transcript_blocks_response.raise_for_status()
                        logging.info(f"âœ… ç¬¬ {batch_num}/{total_batches} æ‰¹é€å­—ç¨¿å…§å®¹æ·»åŠ æˆåŠŸ")
                        break  # Success - exit retry loop
                        
                    except requests.exceptions.RequestException as e:
                        retry_count += 1
                        logging.warning(f"âš ï¸ ç¬¬ {batch_num}/{total_batches} æ‰¹é€å­—ç¨¿æ·»åŠ å¤±æ•— (å˜—è©¦ {retry_count}/{max_retries}): {e}")
                        
                        # Check specific error types
                        if hasattr(e, 'response') and e.response is not None:
                            try:
                                err_details = e.response.json()
                                logging.error(f"   éŒ¯èª¤è©³æƒ…: {json.dumps(err_details, indent=2, ensure_ascii=False)}")
                                
                                # Check for specific Notion API errors
                                if 'message' in err_details and 'block contents are invalid' in err_details['message'].lower():
                                    logging.error("   å¯èƒ½å­˜åœ¨ç„¡æ•ˆçš„å€å¡Šå…§å®¹ï¼Œå˜—è©¦ç°¡åŒ–è™•ç†...")
                                    # Simplify blocks if needed in future iterations
                                
                            except json.JSONDecodeError:
                                logging.error(f"   éŸ¿æ‡‰å…§å®¹ (é JSON): {e.response.text}")
                        
                        if retry_count < max_retries:
                            wait_time = 2 ** retry_count  # Exponential backoff
                            logging.info(f"   ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                            time.sleep(wait_time)
                        else:
                            logging.error(f"âŒ ç¬¬ {batch_num}/{total_batches} æ‰¹é€å­—ç¨¿æ·»åŠ å¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
                
                # Add a small delay between batches to avoid rate limiting
                if i + MAX_BLOCKS_PER_REQUEST < len(remaining_note_blocks) and retry_count < max_retries:
                    time.sleep(1)
            
            logging.info(f"âœ… Notion é é¢å»ºç«‹æˆåŠŸ (ID: {page_id}, URL: {page_url})")
            return page_id, page_url
            
        except requests.exceptions.RequestException as e:
            logging.error(f"âŒ Notion API è«‹æ±‚å¤±æ•—: {e}", exc_info=True)
            if e.response is not None:
                try:
                    err_details = e.response.json()
                    logging.error(f"   éŒ¯èª¤ç¢¼: {e.response.status_code}, è¨Šæ¯: {json.dumps(err_details, indent=2, ensure_ascii=False)}")
                except json.JSONDecodeError:
                    logging.error(f"   éŸ¿æ‡‰å…§å®¹ (é JSON): {e.response.text}")
            raise
        except Exception as e:
            logging.error(f"âŒ Notion é é¢å»ºç«‹æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}", exc_info=True)
            raise

    def load_models(self):
        """è¼‰å…¥æ‰€éœ€çš„ AI æ¨¡å‹"""
        logging.info("ğŸ”„ è¼‰å…¥ AI æ¨¡å‹...")
        
        # è¼‰å…¥ Whisper æ¨¡å‹ (å¦‚æœå°šæœªè¼‰å…¥)
        if self.whisper_model is None:
            try:
                logging.info("- è¼‰å…¥ Whisper æ¨¡å‹ (medium)...")
                self.whisper_model = whisper.load_model("medium")
                logging.info("âœ… Whisper æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            except Exception as e:
                logging.error(f"âŒ Whisper æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                raise
        
        # è¼‰å…¥ Pyannote æ¨¡å‹ (å¦‚æœå°šæœªè¼‰å…¥)
        if self.diarization_pipeline is None:
            # å¢åŠ é‡è©¦æ©Ÿåˆ¶
            max_retries = 3
            retry_count = 0
            last_error = None
            
            while retry_count < max_retries:
                try:
                    logging.info(f"- è¼‰å…¥èªªè©±äººåˆ†é›¢æ¨¡å‹... (å˜—è©¦ {retry_count + 1}/{max_retries})")
                    # Make sure we have the HF_TOKEN
                    hf_token = os.getenv("HF_TOKEN")
                    if not hf_token:
                        raise ValueError("Missing HF_TOKEN environment variable")
                    
                    # Using a specific version instead of latest
                    self.diarization_pipeline = Pipeline.from_pretrained(
                        "pyannote/speaker-diarization-3.1",  # Use specific version
                        use_auth_token=hf_token
                    )
                    logging.info("âœ… èªªè©±äººåˆ†é›¢æ¨¡å‹è¼‰å…¥æˆåŠŸ")
                    break
                except Exception as e:
                    last_error = e
                    logging.error(f"âŒ èªªè©±äººåˆ†é›¢æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                    retry_count += 1
                    time.sleep(2)  # é‡è©¦å‰ç­‰å¾…2ç§’
            
            if self.diarization_pipeline is None:
                logging.error(f"âŒ èªªè©±äººåˆ†é›¢æ¨¡å‹åœ¨ {max_retries} æ¬¡å˜—è©¦å¾Œä»ç„¶è¼‰å…¥å¤±æ•—")
                raise last_error or RuntimeError("Failed to load diarization pipeline")
    
    def convert_to_wav(self, input_path: str) -> str:
        """è½‰æ›æª”æ¡ˆç‚º WAV æ ¼å¼ (16kHz å–®è²é“)"""
        logging.info(f"ğŸ”„ è½‰æ›æª”æ¡ˆæ ¼å¼ç‚º WAV: {os.path.basename(input_path)}")
        
        # ç›®æ¨™è·¯å¾‘ (èˆ‡åŸå§‹æª”æ¡ˆç›¸åŒç›®éŒ„ï¼Œä½†å‰¯æª”åæ”¹ç‚º .wav)
        output_dir = os.path.dirname(input_path)
        output_filename = f"{os.path.splitext(os.path.basename(input_path))[0]}.wav"
        output_path = os.path.join(output_dir, output_filename)
        
        # ä½¿ç”¨ FFmpeg è½‰æ›
        try:
            cmd = [
                "ffmpeg", 
                "-y",                # è¦†è“‹ç¾æœ‰æª”æ¡ˆ
                "-i", input_path,    # è¼¸å…¥æª”æ¡ˆ
                "-ar", "16000",      # æ¡æ¨£ç‡ 16kHz
                "-ac", "1",          # å–®è²é“
                "-c:a", "pcm_s16le", # 16-bit PCM
                output_path          # è¼¸å‡ºæª”æ¡ˆ
            ]
            
            # åŸ·è¡Œå‘½ä»¤
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            logging.info(f"âœ… æª”æ¡ˆè½‰æ›å®Œæˆ: {output_filename}")
            
            return output_path
            
        except subprocess.CalledProcessError as e:
            logging.error(f"âŒ æª”æ¡ˆè½‰æ›å¤±æ•—: {e}")
            raise
    
    def identify_speakers(self, segments: List[Dict[str, Any]], original_speakers: List[str]) -> Dict[str, str]:
        """ä½¿ç”¨ Gemini è¾¨è­˜èªªè©±äººçš„çœŸå¯¦èº«ä»½"""
        logging.info(f"ğŸ”„ è­˜åˆ¥èªªè©±äººèº«ä»½...")
        
        if not segments:
            logging.warning("âš ï¸ æ²’æœ‰èªéŸ³æ®µè½ï¼Œç„¡æ³•è­˜åˆ¥èªªè©±äºº")
            return {}
        
        # æº–å‚™ç¯„ä¾‹å°è©±
        sample_dialogue = ""
        for i, segment in enumerate(segments[:20]):  # æœ€å¤šä½¿ç”¨å‰ 20 å€‹æ®µè½
            speaker = segment["speaker"]
            text = segment["text"]
            sample_dialogue += f"{speaker}: {text}\n"
        
        # æç¤º Gemini è­˜åˆ¥èªªè©±äºº
        try:
            system_prompt = """
            ä½ æ˜¯ä¸€ä½æ–‡å­—è™•ç†å°ˆå®¶ï¼Œå°ˆé–€æ ¹æ“šå°è©±å…§å®¹è¾¨è­˜çœŸå¯¦èªªè©±äººã€‚
            è«‹åˆ†æä»¥ä¸‹å°è©±å…§å®¹ï¼Œè¾¨è­˜å‡ºå„å€‹èªªè©±äººä»£ç¢¼ï¼ˆå¦‚ SPEAKER_00ï¼‰å°æ‡‰çš„æœ€å¯èƒ½çœŸå¯¦å§“åæˆ–è·ç¨±ã€‚
            ä¸ç¢ºå®šçš„èªªè©±äººè«‹ä¿ç•™åŸä»£ç¢¼ã€‚å›æ‡‰æ ¼å¼å¿…é ˆæ˜¯ä¸€å€‹JSONï¼Œkeyç‚ºåŸå§‹èªªè©±äººä»£ç¢¼ï¼Œvalueç‚ºä½ è¾¨è­˜çš„çœŸå¯¦å§“å/è·ç¨±ã€‚
            åªéœ€å›å‚³JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
            """

            response = self.try_multiple_gemini_models(
                system_prompt,
                f"å°è©±å…§å®¹å¦‚ä¸‹ï¼š\n{sample_dialogue}\n\nè«‹è¾¨è­˜å‡ºå„å€‹èªªè©±äººä»£ç¢¼ï¼ˆå¦‚ {', '.join(original_speakers)}ï¼‰å°æ‡‰çš„æœ€å¯èƒ½çœŸå¯¦å§“åæˆ–è·ç¨±ã€‚",
                models=['gemini-2.0-flash', 'gemini-1.5-flash', 'gemini-2.0-flash-lite']
            )
            
            response_text = response.text
            # æœ‰æ™‚ Gemini æœƒåœ¨ JSON å‰å¾ŒåŠ ä¸Šé¡å¤–æ–‡å­—ï¼Œéœ€è¦æå–ç´” JSON éƒ¨åˆ†
            json_match = re.search(r'({.*?})', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            # è§£æ JSON å›æ‡‰
            speaker_map = json.loads(response_text)
            logging.info(f"âœ… èªªè©±äººèº«ä»½è­˜åˆ¥æˆåŠŸ: {speaker_map}")
            
            return speaker_map
            
        except Exception as e:
            logging.error(f"âŒ èªªè©±äººèº«ä»½è­˜åˆ¥å¤±æ•—: {e}")
            return {speaker: speaker for speaker in original_speakers}  # å¤±æ•—æ™‚è¿”å›åŸå§‹ä»£ç¢¼
    
    def generate_summary(self, transcript: str, attachment_text: Optional[str] = None) -> Dict[str, Any]:
        """ä½¿ç”¨ Gemini ç”Ÿæˆæ‘˜è¦ã€æ¨™é¡Œå’Œå¾…è¾¦äº‹é …"""
        logging.info("ğŸ”„ ä½¿ç”¨ Gemini ç”Ÿæˆæ‘˜è¦...")
        
        try:
            context = ""
            if attachment_text:
                context = f"ä»¥ä¸‹æ˜¯æä¾›çš„èƒŒæ™¯è³‡æ–™ï¼š\n{attachment_text}\n\n"
            
            system_prompt = """
            ä½ æ˜¯ä¸€ä½æœƒè­°è¨˜éŒ„å°ˆå®¶ï¼Œå°ˆé•·æ–¼åˆ†ææœƒè­°å…§å®¹ä¸¦ç”¢ç”Ÿé‡é»æ‘˜è¦ã€‚
            åŒæ™‚ä½ å…·å‚™é›»å­å·¥ç¨‹é€šè¨Šç›¸é—œèƒŒæ™¯ï¼Œèƒ½å¤ ç†è§£æŠ€è¡“æ€§å…§å®¹(åŒ…æ‹¬ä¸€äº›å¸¸è½åˆ°çš„socket, RIC, gNB, nFAPI, OAIç­‰è¡“èª)ã€‚
            è«‹åˆ†æä»¥ä¸‹æœƒè­°è¨˜éŒ„ï¼Œä¸¦æä¾›:
            1. ä¸€å€‹ç°¡çŸ­ä¸”æ¸…æ™°çš„æœƒè­°æ¨™é¡Œ
            2. ä¸€æ®µç°¡æ½”çš„æœƒè­°æ‘˜è¦ (ç´„200-300å­—)
            3. ä¸€å€‹å¾…è¾¦äº‹é …æ¸…å–® (åˆ—å‡ºæœƒè­°ä¸­æåˆ°çš„éœ€è¦åŸ·è¡Œçš„é …ç›®)

            å›æ‡‰æ ¼å¼é ˆç‚º JSONï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
            - title: æœƒè­°æ¨™é¡Œ
            - summary: æœƒè­°æ‘˜è¦
            - todos: å¾…è¾¦äº‹é …æ¸…å–® (é™£åˆ—)

            åªéœ€å›å‚³ JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
            """
            
            response = self.try_multiple_gemini_models(
                system_prompt,
                f"{context}ä»¥ä¸‹æ˜¯æœƒè­°è¨˜éŒ„ï¼š\n{transcript}",
                models=
                ['gemini-2.5-flash-preview-04-17',
                        'gemini-1.5-pro', 'gemini-2.0-flash', 'gemini-1.5-flash', 'gemini-2.0-flash-lite']
            )
            
            response_text = response.text
            # æå– JSON éƒ¨åˆ†
            json_match = re.search(r'({.*?})', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            # è§£æ JSON å›æ‡‰
            summary_data = json.loads(response_text)
            logging.info(f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸï¼š{summary_data['title']}")
            
            return summary_data
            
        except Exception as e:
            logging.error(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
            # è¿”å›é è¨­å€¼
            return {
                "title": "æœƒè­°è¨˜éŒ„",
                "summary": "æ‘˜è¦ç”Ÿæˆå¤±æ•—ã€‚" + str(e),
                "todos": ["æª¢æŸ¥æ‘˜è¦ç”Ÿæˆæœå‹™"]
            }

    def process_audio(self, audio_path: str) -> Tuple[str, List[Dict[str, Any]], List[str]]:
        """è™•ç†éŸ³æª”ï¼šé è™•ç†ã€è½‰æ–‡å­—ä¸¦é€²è¡Œèªªè©±äººåˆ†é›¢"""
        logging.info(f"ğŸ”„ è™•ç†éŸ³æª”: {os.path.basename(audio_path)}")
        
        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
        self.load_models()
        
        # å¦‚æœæª”æ¡ˆé WAV æ ¼å¼ï¼Œå…ˆè½‰æ›
        if not audio_path.lower().endswith('.wav'):
            wav_path = self.convert_to_wav(audio_path)
            # ç§»é™¤åŸå§‹æª”æ¡ˆä»¥ç¯€çœç©ºé–“
            os.remove(audio_path)
            audio_path = wav_path
        
        # éŸ³é »é è™•ç† (ç§»é™¤éœéŸ³)
        preprocessed_path = self.preprocess_audio(audio_path)
        if preprocessed_path != audio_path and os.path.exists(audio_path):
            # å¦‚æœç”¢ç”Ÿäº†æ–°çš„è™•ç†æª”æ¡ˆï¼Œå¯ä»¥é¸æ“‡åˆªé™¤åŸå§‹æª”æ¡ˆ
            os.remove(audio_path)
            audio_path = preprocessed_path
            
        # ä½¿ç”¨ Whisper é€²è¡ŒèªéŸ³è½‰æ–‡å­—ï¼Œæ·»åŠ éŒ¯èª¤è™•ç†å’Œå›é€€æ©Ÿåˆ¶
        logging.info("- åŸ·è¡ŒèªéŸ³è½‰æ–‡å­—...")
        asr_result = None
        transcription_attempts = [
            # ç¬¬ä¸€æ¬¡å˜—è©¦ï¼šä½¿ç”¨é è¨­è¨­ç½®
            {"word_timestamps": False, "verbose": False, "model_name": None, "description": "é è¨­è¨­ç½®"},
            # ç¬¬äºŒæ¬¡å˜—è©¦ï¼šä½¿ç”¨è¼ƒå°çš„æ¨¡å‹
            {"word_timestamps": False, "verbose": False, "model_name": "small", "description": "ä½¿ç”¨å°å‹æ¨¡å‹"}
        ]
        
        for i, attempt in enumerate(transcription_attempts):
            try:
                logging.info(f"- å˜—è©¦è½‰éŒ„ ({i+1}/{len(transcription_attempts)}): {attempt['description']}")
                
                # å¦‚æœæŒ‡å®šäº†ä¸åŒçš„æ¨¡å‹å¤§å°ï¼Œè‡¨æ™‚åŠ è¼‰è©²æ¨¡å‹
                temp_model = None
                if attempt['model_name'] and attempt['model_name'] != "medium":
                    temp_model = whisper.load_model(attempt['model_name'])
                    model_to_use = temp_model
                else:
                    model_to_use = self.whisper_model
                
                # åŸ·è¡Œè½‰éŒ„
                asr_result = model_to_use.transcribe(
                    audio_path, 
                    word_timestamps=attempt['word_timestamps'],
                    verbose=attempt['verbose']
                )
                
                # å¦‚æœæˆåŠŸï¼Œé€€å‡ºå˜—è©¦å¾ªç’°
                logging.info(f"âœ… èªéŸ³è½‰éŒ„æˆåŠŸ ({attempt['description']})")
                break
            
            except RuntimeError as e:
                # æª¢æŸ¥æ˜¯å¦æ˜¯å¼µé‡å¤§å°ä¸åŒ¹é…éŒ¯èª¤
                if "must match the size of tensor" in str(e):
                    logging.warning(f"âš ï¸ è½‰éŒ„å¤±æ•— ({attempt['description']}): å¼µé‡å¤§å°ä¸åŒ¹é…ã€‚å˜—è©¦å…¶ä»–åƒæ•¸ã€‚{e}")
                    if i == len(transcription_attempts) - 1:
                        logging.error("âŒ æ‰€æœ‰è½‰éŒ„å˜—è©¦å‡å¤±æ•—")
                    raise
                else:
                    logging.error(f"âŒ è½‰éŒ„æ™‚ç™¼ç”Ÿé‹è¡Œæ™‚éŒ¯èª¤: {e}")
                    raise
            except Exception as e:
                logging.error(f"âŒ è½‰éŒ„å¤±æ•—: {e}")
                raise
        
        if not asr_result:
            raise RuntimeError("ç„¡æ³•è½‰éŒ„éŸ³é »æ–‡ä»¶")
        
        # ä½¿ç”¨ Pyannote é€²è¡Œèªªè©±äººåˆ†é›¢
        logging.info("- åŸ·è¡Œèªªè©±äººåˆ†é›¢...")
        diarization = self.diarization_pipeline(audio_path)
        
        # æ•´åˆçµæœ
        logging.info("- æ•´åˆçµæœ...")
        segments = []
        transcript_full = ""
        original_speakers = set()
        
        # è£½ä½œæ ¼å¼åŒ–çš„è¼¸å‡º
        for i, segment in enumerate(asr_result["segments"]):
            # æ‰¾å‡ºæ­¤æ®µè½çš„ä¸»è¦èªªè©±äºº
            segment_start = segment["start"]
            segment_end = segment["end"]
            text = segment["text"].strip()
            
            # å¾èªªè©±äººåˆ†é›¢çµæœä¸­æ‰¾å‡ºè¦†è“‹æ­¤æ®µè½æ™‚é–“æœ€å¤šçš„èªªè©±äºº
            speakers = {}
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                # è¨ˆç®—é‡ç–Šæ™‚é–“
                overlap_start = max(segment_start, turn.start)
                overlap_end = min(segment_end, turn.end)
                
                if overlap_end > overlap_start:  # æœ‰é‡ç–Š
                    overlap_duration = overlap_end - overlap_start
                    if speaker in speakers:
                        speakers[speaker] += overlap_duration
                    else:
                        speakers[speaker] = overlap_duration
            
            # æ‰¾å‡ºä¸»è¦èªªè©±äºº (è¦†è“‹æ™‚é–“æœ€é•·)
            main_speaker = max(speakers.items(), key=lambda x: x[1])[0] if speakers else "æœªçŸ¥"
            original_speakers.add(main_speaker)
            
            segment_data = {
                "speaker": main_speaker,
                "start": segment_start,
                "end": segment_end,
                "text": text
            }
            
            segments.append(segment_data)
        
        logging.info(f"âœ… éŸ³æª”è™•ç†å®Œæˆï¼Œå…± {len(segments)} å€‹æ®µè½")
        return transcript_full, segments, list(original_speakers)

    def _process_file_job(self, job_id: str, file_id: str, attachment_file_ids: Optional[List[str]] = None):
        """å¾Œå°è™•ç†éŸ³é »æª”æ¡ˆçš„å·¥ä½œå‡½æ•¸ (åœ¨ç·šç¨‹ä¸­åŸ·è¡Œ)"""
        main_temp_dir = None
        attachments_temp_dir = None
        downloaded_pdf_paths = []
        context_summary = ""
        attachment_texts = []

        try:
            logging.info(f"[Job {job_id}] é–‹å§‹è™•ç† file_id: {file_id}")
            
            # ç²å–åŸå§‹æª”æ¡ˆåç¨±
            try:
                file_meta = self.drive_service.files().get(
                    fileId=file_id, fields="name"
                ).execute()
                original_filename = file_meta.get('name', '')
                logging.info(f"[Job {job_id}] åŸå§‹æª”æ¡ˆåç¨±: {original_filename}")
            except Exception as e:
                logging.error(f"[Job {job_id}] âŒ ç²å–åŸå§‹æª”æ¡ˆåç¨±å¤±æ•—: {e}")
                original_filename = ""
            
            # æ›´æ–°é€²åº¦: 10%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 10
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # è™•ç†é™„ä»¶ (å¦‚æœæœ‰)
            if attachment_file_ids:
                attachment_texts = []
                for attachment_file_id in attachment_file_ids:
                    attachment_text, attachment_temp_dir = self.download_and_extract_text(attachment_file_id)
                    attachment_texts.append(attachment_text)
                    if attachment_temp_dir:
                        attachments_temp_dir = attachment_temp_dir
            
            # æ›´æ–°é€²åº¦: 20%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 20
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # ä¸‹è¼‰éŸ³é »æª”æ¡ˆ
            audio_path, audio_temp_dir = self.download_from_drive(file_id)
            
            # æ›´æ–°é€²åº¦: 30%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 30
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # è™•ç†éŸ³é »: è½‰éŒ„å’Œèªªè©±äººåˆ†é›¢
            _, segments, original_speakers = self.process_audio(audio_path)
            
            # æ›´æ–°é€²åº¦: 60%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 60
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
                
            # è­˜åˆ¥èªªè©±äºº
            speaker_map = self.identify_speakers(segments, original_speakers)
            
            # æ›´æ–°é€²åº¦: 70%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 70
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # æº–å‚™è¼¸å‡º
            updated_segments = []
            transcript_for_summary = ""
            if not segments:
                logging.warning(f"[Job {job_id}] âš ï¸ No segments found after audio processing. Transcript will be empty.")
            for seg in segments:
                identified_speaker = speaker_map.get(seg['speaker'], seg['speaker'])
                updated_segments.append({**seg, "speaker": identified_speaker})
                transcript_for_summary += f"[{identified_speaker}]: {seg['text']}\n"
            
            # æ›´æ–°é€²åº¦: 75%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 75
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # ç”Ÿæˆæ‘˜è¦
            summary_data = self.generate_summary(transcript_for_summary, attachment_texts[0] if attachment_texts else None)
            title = summary_data["title"]
            summary = summary_data["summary"]
            todos = summary_data["todos"]
            
            # æ›´æ–°é€²åº¦: 85%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 85
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # å»ºç«‹ Notion é é¢
            page_id, page_url = self.create_notion_page(
                title, summary, todos, updated_segments, speaker_map, file_id
            )
            
            # æ›´æ–°é€²åº¦: 95%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 95
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # é‡å‘½å Google Drive æª”æ¡ˆ (å¯é¸)
            # å¾åŸå§‹æª”åæå–æ—¥æœŸï¼Œå¦‚æœç„¡æ³•æå–å‰‡ä½¿ç”¨ç•¶å‰æ—¥æœŸ
            file_date = None
            if original_filename:
                file_date = self.extract_date_from_filename(original_filename)
            
            date_str = file_date if file_date else datetime.now().strftime('%Y-%m-%d')
            new_filename = f"[{date_str}] {title}.m4a"
            self.rename_drive_file(file_id, new_filename)
            
            # æ›´æ–°å·¥ä½œç‹€æ…‹ç‚ºå®Œæˆ
            result = {
                "success": True,
                "notion_page_id": page_id,
                "notion_page_url": page_url,
                "title": title,
                "summary": summary,
                "todos": todos,
                "identified_speakers": speaker_map,
                "drive_filename": new_filename
            }
            
            with self.jobs_lock:
                self.jobs[job_id]['status'] = JOB_STATUS['COMPLETED']
                self.jobs[job_id]['progress'] = 100
                self.jobs[job_id]['result'] = result
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            logging.info(f"[Job {job_id}] âœ… è™•ç†å®Œæˆ")
            return result

        except Exception as e:
            logging.error(f"[Job {job_id}] âŒ è™•ç†å¤±æ•—: {e}", exc_info=True)
            
            # æº–å‚™éŒ¯èª¤çµæœ
            final_title = summary_data["title"] if 'summary_data' in locals() and summary_data else "è™•ç†å¤±æ•—"
            final_summary = summary_data["summary"] if 'summary_data' in locals() and summary_data else f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}"
            final_todos = summary_data["todos"] if 'summary_data' in locals() and summary_data else ["æª¢æŸ¥è™•ç†æ—¥èªŒ"]
            final_speakers = speaker_map if 'speaker_map' in locals() and speaker_map else None
            
            # æ›´æ–°å·¥ä½œç‹€æ…‹ç‚ºå¤±æ•—
            error_result = {
                "success": False,
                "error": f"è™•ç†å¤±æ•—: {e}",
                "notion_page_id": None,
                "notion_page_url": None,
                "title": final_title,
                "summary": final_summary,
                "todos": final_todos,
                "identified_speakers": final_speakers
            }
            
            with self.jobs_lock:
                self.jobs[job_id]['status'] = JOB_STATUS['FAILED']
                self.jobs[job_id]['progress'] = 100
                self.jobs[job_id]['result'] = error_result
                self.jobs[job_id]['error'] = str(e)
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            return error_result

        finally:
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if audio_temp_dir and os.path.exists(audio_temp_dir):
                logging.info(f"[Job {job_id}] ğŸ§¹ æ¸…ç†éŸ³æª”è‡¨æ™‚ç›®éŒ„")
                shutil.rmtree(audio_temp_dir)
            if attachments_temp_dir and os.path.exists(attachments_temp_dir):
                logging.info(f"[Job {job_id}] ğŸ§¹ æ¸…ç†é™„ä»¶è‡¨æ™‚ç›®éŒ„")
                shutil.rmtree(attachments_temp_dir)

    def process_file_async(self, file_id: str, attachment_file_ids: Optional[List[str]] = None) -> str:
        """éåŒæ­¥è™•ç†æª”æ¡ˆï¼Œè¿”å›å·¥ä½œ ID"""
        # ç”Ÿæˆå”¯ä¸€å·¥ä½œ ID
        job_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–å·¥ä½œç‹€æ…‹
        with self.jobs_lock:
            self.jobs[job_id] = {
                'id': job_id,
                'file_id': file_id,
                'attachment_file_ids': attachment_file_ids,
                'status': JOB_STATUS['PENDING'],
                'progress': 0,
                'created_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat(),
                'result': None,
                'error': None
            }
        
        # æäº¤å·¥ä½œåˆ°ç·šç¨‹æ± 
        self.executor.submit(
            self._process_file_job, job_id, file_id, attachment_file_ids
        )
        
        return job_id
    
    def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """ç²å–å·¥ä½œç‹€æ…‹"""
        with self.jobs_lock:
            job = self.jobs.get(job_id)
            
        if not job:
            return {'error': 'å·¥ä½œä¸å­˜åœ¨'}
        
        # æ ¹æ“šå·¥ä½œç‹€æ…‹è¿”å›ä¸åŒä¿¡æ¯
        if job['status'] == JOB_STATUS['COMPLETED']:
            return {
                'id': job['id'],
                'status': job['status'],
                'progress': job['progress'],
                'created_at': job['created_at'],
                'updated_at': job['updated_at'],
                'result': job['result']
            }
        elif job['status'] == JOB_STATUS['FAILED']:
            return {
                'id': job['id'],
                'status': job['status'],
                'progress': job['progress'],
                'created_at': job['created_at'],
                'updated_at': job['updated_at'],
                'error': job['error']
            }
        else:
            # è™•ç†ä¸­æˆ–ç­‰å¾…ä¸­
            return {
                'id': job['id'],
                'status': job['status'],
                'progress': job['progress'],
                'created_at': job['created_at'],
                'updated_at': job['updated_at']
            }

    def update_job_progress(self, job_id: str, progress: int, message: str, status: Optional[str] = None, error: Optional[str] = None, result_url: Optional[str] = None, notion_page_id: Optional[str] = None):
        """æ›´æ–°æŒ‡å®šå·¥ä½œçš„é€²åº¦ã€ç‹€æ…‹ã€è¨Šæ¯ã€éŒ¯èª¤å’ŒçµæœURL"""
        with self.jobs_lock:
            if job_id in self.jobs:
                self.jobs[job_id]['progress'] = progress
                self.jobs[job_id]['message'] = message
                self.jobs[job_id]['last_updated'] = datetime.utcnow().isoformat() + 'Z'
                if status:
                    self.jobs[job_id]['status'] = status
                if error:
                    self.jobs[job_id]['error'] = error
                if result_url: # Notion page URL or other result link
                    self.jobs[job_id]['result_url'] = result_url
                if notion_page_id:
                    self.jobs[job_id]['notion_page_id'] = notion_page_id
                
                # å¦‚æœç‹€æ…‹æ˜¯å®Œæˆæˆ–å¤±æ•—ï¼Œè¨˜éŒ„å®Œæˆæ™‚é–“
                if status in [JOB_STATUS['COMPLETED'], JOB_STATUS['FAILED']]:
                    self.jobs[job_id]['completed_at'] = datetime.utcnow().isoformat() + 'Z'
                    
                logging.info(f"ğŸ“Š å·¥ä½œé€²åº¦æ›´æ–° - ID: {job_id}, ç‹€æ…‹: {self.jobs[job_id]['status']}, é€²åº¦: {progress}%, è¨Šæ¯: {message}")
            else:
                logging.warning(f"âš ï¸ å˜—è©¦æ›´æ–°ä¸å­˜åœ¨çš„å·¥ä½œ ID: {job_id}")

    def shutdown_executor(self):
        """å„ªé›…åœ°é—œé–‰ ThreadPoolExecutor"""
        if hasattr(self, 'executor') and self.executor:
            logging.info("ğŸ”„ æ­£åœ¨é—œé–‰ AudioProcessor çš„ ThreadPoolExecutor...")
            try:
                # ç­‰å¾…æ‰€æœ‰ç›®å‰æ­£åœ¨åŸ·è¡Œçš„ä»»å‹™å®Œæˆï¼Œä½†ä¸æ¥å—æ–°ä»»å‹™
                self.executor.shutdown(wait=True)
                logging.info("âœ… AudioProcessor çš„ ThreadPoolExecutor å·²æˆåŠŸé—œé–‰ã€‚")
            except Exception as e:
                logging.error(f"âŒ é—œé–‰ AudioProcessor çš„ ThreadPoolExecutor æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)