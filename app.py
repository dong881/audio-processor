import os
import sys
import tempfile
import shutil
import subprocess
import io
import json
import re
import time
import logging  # Import logging
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional

# Flask ç›¸é—œ
from flask import Flask, request, jsonify
import requests

# Google API ç›¸é—œ
from google.oauth2.credentials import Credentials
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# èªéŸ³è™•ç†ç›¸é—œ
import whisper
from pyannote.audio import Pipeline

# LLM API ç›¸é—œ
import google.generativeai as genai

# PDF è™•ç† (éœ€è¦ pip install PyPDF2)
try:
    import PyPDF2
except ImportError:
    print("âš ï¸ PyPDF2 æœªå®‰è£ï¼Œç„¡æ³•è™•ç† PDF é™„ä»¶ã€‚è«‹åŸ·è¡Œ 'pip install PyPDF2'")
    PyPDF2 = None

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
from dotenv import load_dotenv
load_dotenv()

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__)

class AudioProcessor:
    def __init__(self):
        self.whisper_model = None
        self.diarization_pipeline = None
        self.drive_service = None
        self.init_services()

    def init_services(self):
        """åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„æœå‹™"""
        logging.info("ğŸ”„ åˆå§‹åŒ–æœå‹™ä¸­...")
        
        # åˆå§‹åŒ– Google Drive API
        if os.getenv("USE_SERVICE_ACCOUNT") == "true":
            # ä½¿ç”¨æœå‹™å¸³è™Ÿ
            credentials = service_account.Credentials.from_service_account_file(
                os.getenv("GOOGLE_SA_JSON_PATH"),
                scopes=['https://www.googleapis.com/auth/drive.readonly']
            )
        else:
            # ä½¿ç”¨ OAuth æ†‘è­‰
            credentials = Credentials.from_authorized_user_file(
                os.getenv("GOOGLE_CREDS_JSON_PATH"),
                scopes=['https://www.googleapis.com/auth/drive.readonly']
            )
        
        self.drive_service = build('drive', 'v3', credentials=credentials)
        
        # åˆå§‹åŒ– Google Gemini API
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        
        logging.info("âœ… æœå‹™åˆå§‹åŒ–å®Œæˆ")

    def load_models(self):
        """æ‡¶åŠ è¼‰ AI æ¨¡å‹ï¼Œç¯€çœè¨˜æ†¶é«”"""
        logging.info("ğŸ”„ è¼‰å…¥AIæ¨¡å‹...")
        
        # è¼‰å…¥ Whisper æ¨¡å‹ (å¦‚æœå°šæœªè¼‰å…¥)
        if self.whisper_model is None:
            logging.info("- è¼‰å…¥ Whisper æ¨¡å‹...")
            self.whisper_model = whisper.load_model("base")
            logging.info("- Whisper æ¨¡å‹è¼‰å…¥å®Œæˆ")
        
        # è¼‰å…¥ Pyannote æ¨¡å‹ (å¦‚æœå°šæœªè¼‰å…¥)
        if self.diarization_pipeline is None:
            logging.info("- è¼‰å…¥ Pyannote èªªè©±äººåˆ†é›¢æ¨¡å‹...")
            self.diarization_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=os.getenv("HF_TOKEN")
            )
            logging.info("- Pyannote æ¨¡å‹è¼‰å…¥å®Œæˆ")
            
        logging.info("âœ… æ‰€æœ‰AIæ¨¡å‹è¼‰å…¥å®Œæˆ")

    def download_from_drive(self, file_id: str) -> Tuple[str, str]:
        """å¾ Google Drive ä¸‹è¼‰æª”æ¡ˆåˆ°è‡¨æ™‚ç›®éŒ„"""
        logging.info(f"ğŸ”„ å¾ Google Drive ä¸‹è¼‰æª”æ¡ˆ (ID: {file_id})...")

        # å»ºç«‹è‡¨æ™‚ç›®éŒ„
        temp_dir = tempfile.mkdtemp()

        try:
            # å–å¾—æª”æ¡ˆè³‡è¨Š
            file_meta = self.drive_service.files().get(
                fileId=file_id, fields="name,mimeType"
            ).execute()
            
            filename = file_meta.get('name', f'audio_{file_id}')
            mime_type = file_meta.get('mimeType', '')
            
            logging.info(f"- æª”æ¡ˆåç¨±: {filename}")
            logging.info(f"- MIMEé¡å‹: {mime_type}")
            
            # å»ºç«‹æœ¬åœ°æª”æ¡ˆè·¯å¾‘
            local_path = os.path.join(temp_dir, filename)
            
            # ä¸‹è¼‰æª”æ¡ˆ
            request = self.drive_service.files().get_media(fileId=file_id)
            with io.FileIO(local_path, 'wb') as fh:
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
                    logging.info(f"- ä¸‹è¼‰é€²åº¦: {int(status.progress() * 100)}%")
            
            logging.info(f"âœ… æª”æ¡ˆä¸‹è¼‰å®Œæˆ: {local_path}")
            return local_path, temp_dir
            
        except Exception as e:
            logging.error(f"âŒ æª”æ¡ˆä¸‹è¼‰å¤±æ•—: {str(e)}")
            shutil.rmtree(temp_dir)
            raise

    def convert_to_wav(self, input_path: str) -> str:
        """å°‡éŸ³æª”è½‰æ›ç‚º WAV æ ¼å¼ (16kHz, å–®è²é“)"""
        logging.info(f"ğŸ”„ è½‰æ›æª”æ¡ˆæ ¼å¼: {os.path.basename(input_path)} -> WAV")
        
        # åœ¨ç›¸åŒç›®éŒ„ä¸­å»ºç«‹æš«å­˜ WAV æª”æ¡ˆ
        dir_path = os.path.dirname(input_path)
        temp_wav = tempfile.NamedTemporaryFile(
            suffix=".wav", delete=False, dir=dir_path
        )
        temp_wav.close()
        
        # ä½¿ç”¨ FFmpeg é€²è¡Œè½‰æ›
        cmd = [
            'ffmpeg', '-y',
            '-i', input_path,
            '-acodec', 'pcm_s16le',  # 16-bit PCM
            '-ar', '16000',          # 16kHz å–æ¨£ç‡
            '-ac', '1',              # å–®è²é“
            temp_wav.name
        ]
        
        try:
            result = subprocess.run(
                cmd, 
                check=True, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE
            )
            logging.info(f"âœ… è½‰æ›æˆåŠŸ: {temp_wav.name}")
            return temp_wav.name
        except subprocess.CalledProcessError as e:
            logging.error(f"âŒ è½‰æ›å¤±æ•—: {str(e)}")
            logging.error(f"FFmpeg stderr: {e.stderr.decode('utf-8', errors='replace')}")
            os.remove(temp_wav.name)
            raise RuntimeError("éŸ³æª”è½‰æ›å¤±æ•—")

    def process_audio(self, audio_path: str) -> Tuple[str, List[Dict[str, Any]], List[str]]:
        """è™•ç†éŸ³æª”ï¼šè½‰æ–‡å­—ä¸¦é€²è¡Œèªªè©±äººåˆ†é›¢"""
        logging.info(f"ğŸ”„ è™•ç†éŸ³æª”: {os.path.basename(audio_path)}")
        
        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
        self.load_models()
        
        # å¦‚æœæª”æ¡ˆé WAV æ ¼å¼ï¼Œå…ˆè½‰æ›
        if not audio_path.lower().endswith('.wav'):
            wav_path = self.convert_to_wav(audio_path)
            # å¯é¸ï¼šç§»é™¤åŸå§‹æª”æ¡ˆä»¥ç¯€çœç©ºé–“
            os.remove(audio_path)
            audio_path = wav_path
            
        # ä½¿ç”¨ Whisper é€²è¡ŒèªéŸ³è½‰æ–‡å­—
        logging.info("- åŸ·è¡ŒèªéŸ³è½‰æ–‡å­—...")
        asr_result = self.whisper_model.transcribe(
            audio_path, 
            word_timestamps=True,  # å•Ÿç”¨å­—è©æ™‚é–“æˆ³è¨˜
            verbose=False
        )
        
        # ä½¿ç”¨ Pyannote é€²è¡Œèªªè©±äººåˆ†é›¢
        logging.info("- åŸ·è¡Œèªªè©±äººåˆ†é›¢...")
        diarization = self.diarization_pipeline(audio_path)
        
        # æ•´åˆçµæœ
        logging.info("- æ•´åˆçµæœ...")
        segments = []
        transcript_full = ""
        original_speakers = set()
        
        # è£½ä½œæ ¼å¼åŒ–çš„è¼¸å‡º
        for i, segment in enumerate(asr_result["segments"]):
            # æ‰¾å‡ºæ­¤æ®µè½çš„ä¸»è¦èªªè©±äºº
            segment_start = segment["start"]
            segment_end = segment["end"]
            text = segment["text"].strip()
            
            # å¾èªªè©±äººåˆ†é›¢çµæœä¸­æ‰¾å‡ºè¦†è“‹æ­¤æ®µè½æ™‚é–“æœ€å¤šçš„èªªè©±äºº
            speakers = {}
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                # è¨ˆç®—é‡ç–Šæ™‚é–“
                overlap_start = max(segment_start, turn.start)
                overlap_end = min(segment_end, turn.end)
                
                if overlap_end > overlap_start:  # æœ‰é‡ç–Š
                    overlap_duration = overlap_end - overlap_start
                    if speaker in speakers:
                        speakers[speaker] += overlap_duration
                    else:
                        speakers[speaker] = overlap_duration
            
            # æ‰¾å‡ºä¸»è¦èªªè©±äºº (è¦†è“‹æ™‚é–“æœ€é•·)
            main_speaker = max(speakers.items(), key=lambda x: x[1])[0] if speakers else "æœªçŸ¥"
            original_speakers.add(main_speaker)
            
            segment_data = {
                "speaker": main_speaker,
                "start": segment_start,
                "end": segment_end,
                "text": text
            }
            
            segments.append(segment_data)
        
        logging.info(f"âœ… éŸ³æª”è™•ç†å®Œæˆï¼Œå…± {len(segments)} å€‹æ®µè½")
        return transcript_full, segments, list(original_speakers)

    def identify_speakers(self, segments: List[Dict[str, Any]], original_speakers: List[str]) -> Dict[str, str]:
        """ä½¿ç”¨ Gemini å˜—è©¦è­˜åˆ¥èªªè©±äººåç¨±"""
        logging.info("ğŸ”„ å˜—è©¦è­˜åˆ¥èªªè©±äºº...")
        
        if not original_speakers or "æœªçŸ¥" in original_speakers:
            logging.info("- ç„¡æ³•è­˜åˆ¥ 'æœªçŸ¥' èªªè©±äººï¼Œè·³éè­˜åˆ¥ã€‚")
            return {spk: spk for spk in original_speakers}
        
        # çµ„åˆå°è©±å…§å®¹çµ¦ LLM
        conversation = ""
        for seg in segments:
            conversation += f"[{seg['speaker']}] {seg['text']}\n"
        
        prompt = f"""
        ä»¥ä¸‹æ˜¯ä¸€æ®µå°è©±è¨˜éŒ„ï¼Œå…¶ä¸­èªªè©±äººè¢«æ¨™è¨˜ç‚º {', '.join(original_speakers)}ã€‚
        è«‹åˆ†æå°è©±å…§å®¹ï¼Œåˆ¤æ–·æ¯å€‹æ¨™ç±¤ï¼ˆä¾‹å¦‚ SPEAKER_00, SPEAKER_01ï¼‰å¯¦éš›ä»£è¡¨çš„äººåæ˜¯èª°ã€‚
        äººåå¯èƒ½åœ¨å°è©±ä¸­è¢«ç›´æ¥æåŠã€‚

        å°è©±è¨˜éŒ„ï¼š
        {conversation}

        è«‹æ ¹æ“šä½ çš„åˆ†æï¼Œæä¾›ä¸€å€‹ JSON æ ¼å¼çš„æ˜ å°„ï¼Œå°‡åŸå§‹æ¨™ç±¤æ˜ å°„åˆ°è­˜åˆ¥å‡ºçš„äººåã€‚
        å¦‚æœç„¡æ³•å¾å°è©±ä¸­ç¢ºå®šæŸå€‹æ¨™ç±¤çš„äººåï¼Œè«‹ä¿ç•™åŸå§‹æ¨™ç±¤ã€‚
        ç¯„ä¾‹ï¼š{{ "SPEAKER_00": "å¼µä¸‰", "SPEAKER_01": "SPEAKER_01" }}

        è«‹åªå›è¦† JSON æ ¼å¼çš„æ˜ å°„ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—æˆ–è§£é‡‹ã€‚
        """
        
        try:
            # Use the latest flash model identifier
            model = genai.GenerativeModel('gemini-1.5-flash-latest')
            response = model.generate_content(prompt)
            
            # æ¸…ç†ä¸¦è§£æ JSON å›æ‡‰
            cleaned_text = response.text.strip().lstrip('```json').rstrip('```').strip()
            speaker_map = json.loads(cleaned_text)
            
            # é©—è­‰æ ¼å¼æ˜¯å¦æ­£ç¢º
            if not isinstance(speaker_map, dict):
                raise ValueError("LLM å›æ‡‰ä¸æ˜¯æœ‰æ•ˆçš„ JSON å°è±¡")
            for key in original_speakers:
                if key not in speaker_map:
                    logging.warning(f"âš ï¸ LLM å›æ‡‰ç¼ºå°‘æ¨™ç±¤ '{key}'ï¼Œå°‡ä½¿ç”¨åŸå§‹æ¨™ç±¤ã€‚")
                    speaker_map[key] = key
                elif not isinstance(speaker_map[key], str):
                    logging.warning(f"âš ï¸ LLM å›æ‡‰ä¸­æ¨™ç±¤ '{key}' çš„å€¼ä¸æ˜¯å­—ä¸²ï¼Œå°‡ä½¿ç”¨åŸå§‹æ¨™ç±¤ã€‚")
                    speaker_map[key] = key
            
            logging.info(f"âœ… èªªè©±äººè­˜åˆ¥å®Œæˆ: {speaker_map}")
            return speaker_map
        except Exception as e:
            logging.error(f"âŒ èªªè©±äººè­˜åˆ¥å¤±æ•—: {str(e)}")
            # Log API feedback if available (check response existence)
            if 'response' in locals() and hasattr(response, 'prompt_feedback'):
                logging.error(f"   - Gemini Prompt Feedback: {response.prompt_feedback}")
            if 'response' in locals() and response and response.candidates:
                logging.error(f"   - Gemini Finish Reason: {response.candidates[0].finish_reason}")
            return {spk: spk for spk in original_speakers}

    def generate_summary(self, transcript: str, attachment_text: Optional[str] = None) -> Dict[str, str]:
        """ä½¿ç”¨ Gemini ç”Ÿæˆæ‘˜è¦å’Œå¾…è¾¦äº‹é …ï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶å’Œæ›´è©³ç´°çš„æ—¥èªŒè¨˜éŒ„"""
        logging.info("ğŸ”„ é–‹å§‹ç”Ÿæˆæ‘˜è¦èˆ‡å¾…è¾¦äº‹é …...")

        # Check if the transcript is empty or too short
        if not transcript or len(transcript.strip()) < 10:
            logging.warning("âš ï¸ å‚³å…¥çš„ transcript ç‚ºç©ºæˆ–éçŸ­ï¼Œç„¡æ³•ç”Ÿæˆæ‘˜è¦ã€‚")
            return self.get_fallback_summary_data("Transcript is empty or too short")

        # Log the first few characters of the transcript to verify content
        logging.info(f"  - Transcript (start): {transcript[:200]}...")
        if attachment_text:
            logging.info(f"  - Attachment Text (start): {attachment_text[:200]}...")

        # --- Prompt Definition ---
        prompt_parts = [
            "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æœƒè­°è¨˜éŒ„å“¡ã€‚æ ¹æ“šä»¥ä¸‹æä¾›çš„èªéŸ³è¨˜éŒ„",
        ]
        if attachment_text:
            prompt_parts.append("å’Œé™„åŠ æ–‡ä»¶å…§å®¹")
        prompt_parts.extend([
            f"""
            ï¼Œè«‹åŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š
            1. æ’°å¯«ä¸€æ®µä¸è¶…é300å­—çš„æ‘˜è¦ã€‚
            2. åˆ—å‡ºä¸è¶…é5é …çš„é‡è¦å¾…è¾¦äº‹é … (To-Do)ã€‚
            3. çµ¦é€™å€‹æœƒè­°ä¸€å€‹ç°¡çŸ­ä½†æè¿°æ€§å¼·çš„æ¨™é¡Œã€‚

            èªéŸ³è¨˜éŒ„ï¼š
            {transcript}
            """
        ])
        if attachment_text:
            prompt_parts.append(f"\né™„åŠ æ–‡ä»¶å…§å®¹ï¼š\n{attachment_text}\n")
        prompt_parts.append(
            """
            **é‡è¦æŒ‡ç¤ºï¼š** ä½ çš„å›è¦† **å¿…é ˆ** åƒ…åŒ…å«ä¸€å€‹æœ‰æ•ˆçš„ JSON ç‰©ä»¶ï¼Œå…¶çµæ§‹å¦‚ä¸‹æ‰€ç¤ºã€‚
            **çµ•å°ä¸è¦** åœ¨ JSON ç‰©ä»¶å‰å¾ŒåŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€è¨»è§£ã€èªªæ˜æˆ– markdown æ¨™è¨˜ (ä¾‹å¦‚ ```json ... ```)ã€‚

            ```json
            {{
                "title": "æœƒè­°æ¨™é¡Œ",
                "summary": "æœƒè­°æ‘˜è¦...",
                "todos": ["å¾…è¾¦äº‹é …1", "å¾…è¾¦äº‹é …2", ...]
            }}
            ```
            """
        )
        full_prompt = "".join(prompt_parts)
        # --- End Prompt Definition ---

        max_retries = 3
        retry_delay = 2  # seconds

        for attempt in range(max_retries):
            logging.info(f"  - å˜—è©¦èª¿ç”¨ Gemini API (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)...")
            response = None  # Initialize response to None
            try:
                # Use the latest flash model identifier
                model = genai.GenerativeModel('gemini-2.0-flash')
                # model = genai.GenerativeModel('models/gemini-2.5-flash-preview-04-17')
                response = model.generate_content(full_prompt)
                raw_text = response.text

                logging.info(f"  - Gemini Raw Response (Attempt {attempt+1}):\n{raw_text}")  # Log the FULL raw response

                # --- JSON Parsing Logic ---
                cleaned_text = raw_text.strip()
                match = re.search(r"```json\s*(\{.*?\})\s*```", cleaned_text, re.DOTALL)
                if match:
                    json_text = match.group(1)
                    logging.info("  - JSON extracted from markdown block.")
                else:
                    if cleaned_text.startswith('{') and cleaned_text.endswith('}'):
                        json_text = cleaned_text
                        logging.info("  - Raw response treated as JSON.")
                    else:
                        logging.error(f"  - è§£æå¤±æ•—ï¼šå›æ‡‰å…§å®¹ä¸æ˜¯é æœŸçš„ JSON ç‰©ä»¶æ ¼å¼æˆ– Markdown JSON å€å¡Šã€‚")
                        raise ValueError("Response content was not a valid JSON object or markdown block.")

                summary_data = json.loads(json_text)

                if not all(k in summary_data for k in ["title", "summary", "todos"]):
                    logging.error(f"  - è§£æå¤±æ•—ï¼šJSON ç‰©ä»¶ç¼ºå°‘å¿…è¦çš„éµ (title, summary, todos)ã€‚ Found keys: {list(summary_data.keys())}")
                    raise ValueError("JSON object missing required keys (title, summary, todos)")
                if not isinstance(summary_data["todos"], list):
                    logging.error(f"  - è§£æå¤±æ•—ï¼š'todos' éµçš„å€¼ä¸æ˜¯åˆ—è¡¨ã€‚ Type: {type(summary_data['todos'])}")
                    raise ValueError("'todos' key value must be a list")
                # --- End JSON Parsing Logic ---

                logging.info("âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸ")
                return summary_data

            except json.JSONDecodeError as json_err:
                logging.error(f"âŒ ç¬¬ {attempt + 1} æ¬¡ JSON è§£æå¤±æ•—: {str(json_err)}")
                logging.error(f"   - Failed JSON Text: {json_text[:500]}...")  # Log the text that failed parsing

            except Exception as e:
                logging.error(f"âŒ ç¬¬ {attempt + 1} æ¬¡æ‘˜è¦ç”Ÿæˆ/è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                # Check if response exists before accessing attributes
                if response:
                    if hasattr(response, 'prompt_feedback'):
                        logging.error(f"   - Gemini Prompt Feedback: {response.prompt_feedback}")
                    if response.candidates:
                        logging.error(f"   - Gemini Finish Reason: {response.candidates[0].finish_reason}")
                else:
                    logging.error("   - Gemini API call likely failed before response was received.")

            if attempt < max_retries - 1:
                logging.info(f"   å°‡åœ¨ {retry_delay} ç§’å¾Œé‡è©¦...")
                time.sleep(retry_delay)
            else:
                logging.error("âŒ å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ‘˜è¦ç”Ÿæˆå¤±æ•—ã€‚")

        logging.warning("âŒ æ‰€æœ‰æ‘˜è¦ç”Ÿæˆå˜—è©¦å¤±æ•—ï¼Œè¿”å›é è¨­å…§å®¹ã€‚")
        return self.get_fallback_summary_data("Max retries reached or permanent error")

    def get_fallback_summary_data(self, reason: str = "Unknown error") -> Dict[str, str]:
        """Returns the default summary data when generation fails."""
        logging.warning(f"  - Using fallback summary data. Reason: {reason}")
        return {
            "title": "æœƒè­°è¨˜éŒ„ (æ‘˜è¦ç”Ÿæˆå¤±æ•—)",
            "summary": "ç„¡æ³•ç”Ÿæˆæ‘˜è¦ï¼Œè«‹æŸ¥çœ‹åŸå§‹è¨˜éŒ„ã€‚",
            "todos": ["æª¢é–±æœƒè­°è¨˜éŒ„ä¸¦æ‰‹å‹•æ•´ç†é‡é»"]
        }

    def create_notion_page(self, title: str, summary: str, todos: List[str], segments: List[Dict[str, Any]]) -> Tuple[str, str]:
        """å»ºç«‹ Notion é é¢ (ç§»é™¤é€å­—ç¨¿æ™‚é–“æˆ³)"""
        logging.info("ğŸ”„ å»ºç«‹ Notion é é¢...")

        notion_token = os.getenv("NOTION_TOKEN")
        database_id = os.getenv("NOTION_DATABASE_ID")

        if not notion_token or not database_id:
            raise ValueError("ç¼ºå°‘ Notion API è¨­å®š")

        blocks = []

        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "æ‘˜è¦"}}]
            }
        })
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [{"type": "text", "text": {"content": summary}}]
            }
        })

        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "å¾…è¾¦äº‹é …"}}]
            }
        })

        for todo in todos:
            blocks.append({
                "object": "block",
                "type": "to_do",
                "to_do": {
                    "rich_text": [{"type": "text", "text": {"content": todo}}],
                    "checked": False
                }
            })

        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "å®Œæ•´è¨˜éŒ„"}}]
            }
        })

        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
                "rich_text": [{"type": "text", "text": {"content": "ä»¥ä¸‹æ˜¯æœƒè­°çš„å®Œæ•´è½‰éŒ„å…§å®¹ï¼š"}}]
            }
        })

        for segment in segments:
            speaker = segment["speaker"]
            text = segment["text"]

            content = f"[{speaker}]: {text}"

            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": content}}]
                }
            })

        headers = {
            "Authorization": f"Bearer {notion_token}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28"
        }

        current_date = datetime.now().strftime("%Y-%m-%d")
        data = {
            "parent": {"database_id": database_id},
            "properties": {
                "title": {
                    "title": [{"text": {"content": f"{title} ({current_date})"}}]
                }
            },
            "children": blocks
        }

        try:
            response = requests.post(
                "https://api.notion.com/v1/pages",
                headers=headers,
                json=data
            )
            response.raise_for_status()
            result = response.json()
            page_id = result["id"]
            page_url = result.get("url", f"https://www.notion.so/{page_id.replace('-', '')}")
            logging.info(f"âœ… Notion é é¢å»ºç«‹æˆåŠŸ (ID: {page_id}, URL: {page_url})")
            return page_id, page_url
        except requests.exceptions.RequestException as e:
            logging.error(f"âŒ Notion API è«‹æ±‚å¤±æ•—: {str(e)}", exc_info=True)
            if e.response is not None:
                try:
                    err_details = e.response.json()
                    logging.error(f"   éŒ¯èª¤ç¢¼: {e.response.status_code}, è¨Šæ¯: {json.dumps(err_details, indent=2, ensure_ascii=False)}")
                except json.JSONDecodeError:
                    logging.error(f"   éŸ¿æ‡‰å…§å®¹ (é JSON): {e.response.text}")
            raise
        except Exception as e:
            logging.error(f"âŒ Notion é é¢å»ºç«‹æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {str(e)}", exc_info=True)
            raise

    def process_file(self, file_id: str, attachment_file_id: Optional[str] = None) -> Dict[str, Any]:
        """è™•ç†å®Œæ•´æµç¨‹ï¼ŒåŒ…å«é™„ä»¶å’Œèªªè©±äººè­˜åˆ¥"""
        audio_temp_dir = None
        attachment_temp_dir = None
        attachment_text = None
        summary_data = None

        try:
            logging.info(f"Processing file_id: {file_id}, attachment_file_id: {attachment_file_id}")
            if attachment_file_id:
                attachment_text, attachment_temp_dir = self.download_and_extract_text(attachment_file_id)

            audio_path, audio_temp_dir = self.download_from_drive(file_id)

            _, segments, original_speakers = self.process_audio(audio_path)

            speaker_map = self.identify_speakers(segments, original_speakers)

            updated_segments = []
            transcript_for_summary = ""
            if not segments:
                logging.warning("âš ï¸ No segments found after audio processing. Transcript will be empty.")
            for seg in segments:
                identified_speaker = speaker_map.get(seg['speaker'], seg['speaker'])
                updated_segments.append({**seg, "speaker": identified_speaker})
                transcript_for_summary += f"[{identified_speaker}]: {seg['text']}\n"

            summary_data = self.generate_summary(transcript_for_summary, attachment_text)
            title = summary_data["title"]
            summary = summary_data["summary"]
            todos = summary_data["todos"]

            page_id, page_url = self.create_notion_page(title, summary, todos, updated_segments)

            logging.info(f"âœ… File processing successful for file_id: {file_id}")
            return {
                "success": True,
                "notion_page_id": page_id,
                "notion_page_url": page_url,
                "title": title,
                "summary": summary,
                "todos": todos,
                "identified_speakers": speaker_map
            }

        except Exception as e:
            logging.error(f"è™•ç†æª”æ¡ˆæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ (file_id: {file_id}): {str(e)}", exc_info=True)
            final_title = summary_data["title"] if summary_data else "è™•ç†å¤±æ•—"
            final_summary = summary_data["summary"] if summary_data else f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            final_todos = summary_data["todos"] if summary_data else ["æª¢æŸ¥è™•ç†æ—¥èªŒ"]

            return {
                "success": False,
                "error": f"è™•ç†å¤±æ•—: {str(e)}",
                "notion_page_id": None,
                "notion_page_url": None,
                "title": final_title,
                "summary": final_summary,
                "todos": final_todos,
                "identified_speakers": None
            }

        finally:
            if audio_temp_dir and os.path.exists(audio_temp_dir):
                logging.info(f"ğŸ§¹ æ¸…ç†éŸ³æª”è‡¨æ™‚ç›®éŒ„: {audio_temp_dir}")
                shutil.rmtree(audio_temp_dir)
            if attachment_temp_dir and os.path.exists(attachment_temp_dir):
                logging.info(f"ğŸ§¹ æ¸…ç†é™„ä»¶è‡¨æ™‚ç›®éŒ„: {attachment_temp_dir}")
                shutil.rmtree(attachment_temp_dir)

processor = AudioProcessor()

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return jsonify({"status": "healthy", "timestamp": datetime.now().isoformat()})

@app.route('/process', methods=['POST'])
def process_audio_endpoint():
    """è™•ç†éŸ³æª”çš„ API ç«¯é»ï¼Œå¯é¸é™„ä»¶"""
    try:
        data = request.json

        if not data:
            return jsonify({"success": False, "error": "ç„¡æ•ˆçš„è«‹æ±‚å…§å®¹"}), 400

        file_id = data.get('file_id')
        attachment_file_id = data.get('attachment_file_id')

        if not file_id:
            return jsonify({"success": False, "error": "ç¼ºå°‘ file_id åƒæ•¸"}), 400

        result = processor.process_file(file_id, attachment_file_id)

        if result.get("success"):
            return jsonify(result)
        else:
            logging.error(f"è™•ç†å¤±æ•—ï¼ŒéŒ¯èª¤: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            return jsonify({"success": False, "error": result.get('error', 'è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤')}), 500

    except Exception as e:
        logging.error(f"API éŒ¯èª¤: {str(e)}", exc_info=True)
        return jsonify({"success": False, "error": f"ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {str(e)}"}), 500

if __name__ == "__main__":
    port = int(os.getenv("PORT", 5000))
    debug = os.getenv("FLASK_DEBUG", "false").lower() == "true"
    
    logging.info(f"ğŸš€ å•Ÿå‹•ä¼ºæœå™¨æ–¼ port {port}...")
    app.run(host='0.0.0.0', port=port, debug=debug)
