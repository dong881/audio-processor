import os
import sys
import tempfile
import shutil
import subprocess
import io
import json
import re
import time
import logging
import uuid
import threading
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from concurrent.futures import ThreadPoolExecutor

# Flask ç›¸é—œ
from flask import Flask, request, jsonify, render_template, redirect, url_for, session, send_from_directory
import requests

# Google API ç›¸é—œ
from google.oauth2.credentials import Credentials
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# èªéŸ³è™•ç†ç›¸é—œ
import whisper
from pyannote.audio import Pipeline
import numpy as np
import soundfile as sf
import librosa

# LLM API ç›¸é—œ
import google.generativeai as genai

# æ·»åŠ å°å…¥ NotionFormatter
from notion_formatter import NotionFormatter

# PDF è™•ç† (éœ€è¦ pip install PyPDF2)
try:
    import PyPDF2
except ImportError:
    print("âš ï¸ PyPDF2 æœªå®‰è£ï¼Œç„¡æ³•è™•ç† PDF é™„ä»¶ã€‚è«‹åŸ·è¡Œ 'pip install PyPDF2'")
    PyPDF2 = None

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
from dotenv import load_dotenv
import re
load_dotenv()

# Configure basic logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = Flask(__name__, 
            static_folder='static',
            template_folder='templates')
app.secret_key = os.getenv('FLASK_SECRET_KEY', 'dev_secret_key')  # ç‚ºsessionè¨­ç½®å¯†é‘°

# å·¥ä½œç‹€æ…‹å®šç¾©
JOB_STATUS = {
    'PENDING': 'pending',     # ç­‰å¾…è™•ç†
    'PROCESSING': 'processing', # è™•ç†ä¸­
    'COMPLETED': 'completed',   # è™•ç†å®Œæˆ
    'FAILED': 'failed'        # è™•ç†å¤±æ•—
}

class AudioProcessor:
    def __init__(self, max_workers=3):
        self.whisper_model = None
        self.diarization_pipeline = None
        self.drive_service = None
        # åˆå§‹åŒ–åŸ·è¡Œç·’æ± 
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        # å·¥ä½œç‹€æ…‹è¿½è¹¤
        self.jobs = {}
        # ç¢ºä¿ç·šç¨‹å®‰å…¨çš„é–
        self.jobs_lock = threading.Lock()
        # åˆå§‹åŒ– Notion æ ¼å¼åŒ–å·¥å…·
        self.notion_formatter = NotionFormatter()
        # åˆå§‹åŒ–æœå‹™
        self.init_services()

    def init_services(self):
        """åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„æœå‹™"""
        logging.info("ğŸ”„ åˆå§‹åŒ–æœå‹™ä¸­...")
        
        # åˆå§‹åŒ– Google Drive API
        if os.getenv("USE_SERVICE_ACCOUNT") == "true":
            # ä½¿ç”¨æœå‹™å¸³è™Ÿ
            credentials = service_account.Credentials.from_service_account_file(
                os.getenv("GOOGLE_SA_JSON_PATH"),
                scopes=['https://www.googleapis.com/auth/drive']  # å¢åŠ å¯«å…¥æ¬Šé™
            )
        else:
            # ä½¿ç”¨ OAuth æ†‘è­‰
            credentials = Credentials.from_authorized_user_file(
                os.getenv("GOOGLE_CREDS_JSON_PATH"),
                scopes=['https://www.googleapis.com/auth/drive']  # å¢åŠ å¯«å…¥æ¬Šé™
            )
        
        self.drive_service = build('drive', 'v3', credentials=credentials)
        
        # åˆå§‹åŒ– Google Gemini API
        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
        
        logging.info("âœ… æœå‹™åˆå§‹åŒ–å®Œæˆ")
    
    def download_from_drive(self, file_id: str) -> Tuple[str, str]:
        """å¾ Google Drive ä¸‹è¼‰æª”æ¡ˆåˆ°è‡¨æ™‚ç›®éŒ„"""
        logging.info(f"ğŸ”„ å¾ Google Drive ä¸‹è¼‰æª”æ¡ˆ (ID: {file_id})")
        
        try:
            # å»ºç«‹è‡¨æ™‚ç›®éŒ„
            temp_dir = tempfile.mkdtemp()
            
            # ç²å–æ–‡ä»¶è³‡è¨Š
            file_meta = self.drive_service.files().get(
                fileId=file_id, fields="name,mimeType"
            ).execute()
            
            # ç²å–æª”æ¡ˆåç¨±ä¸¦æ¸…ç†ä¸å®‰å…¨çš„å­—å…ƒ
            raw_file_name = file_meta.get('name', f"file_{file_id}")
            # ç§»é™¤æ–œç·šç­‰ä¸å®‰å…¨å­—å…ƒï¼Œé¿å…è·¯å¾‘å•é¡Œ
            safe_file_name = re.sub(r'[\\/*?:"<>|]', "_", raw_file_name)
            local_path = os.path.join(temp_dir, safe_file_name)
            
            # ä¸‹è¼‰æª”æ¡ˆ
            request = self.drive_service.files().get_media(fileId=file_id)
            
            with open(local_path, 'wb') as f:
                downloader = MediaIoBaseDownload(f, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
                    logging.debug(f"ä¸‹è¼‰é€²åº¦: {int(status.progress() * 100)}%")
            
            logging.info(f"âœ… æª”æ¡ˆä¸‹è¼‰å®Œæˆ: {safe_file_name} (å„²å­˜æ–¼ {temp_dir})")
            return local_path, temp_dir
            
        except Exception as e:
            logging.error(f"âŒ ä¸‹è¼‰æª”æ¡ˆå¤±æ•—: {str(e)}")
            if 'temp_dir' in locals() and os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
            raise
    
    def download_and_extract_text(self, file_id: str) -> Tuple[Optional[str], Optional[str]]:
        """ä¸‹è¼‰ä¸¦æå– PDF æ–‡å­—å…§å®¹"""
        try:
            # ç²å–æ–‡ä»¶è³‡è¨Š
            file_meta = self.drive_service.files().get(
                fileId=file_id, fields="name,mimeType"
            ).execute()
            
            mime_type = file_meta.get('mimeType', '')
            
            # ç›®å‰åƒ…æ”¯æŒ PDF
            if mime_type != 'application/pdf' or PyPDF2 is None:
                return None, None
            
            # ä¸‹è¼‰æ–‡ä»¶
            local_path, temp_dir = self.download_from_drive(file_id)
            
            # æå– PDF æ–‡å­—
            text = ""
            with open(local_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    text += page.extract_text()
            
            return text, temp_dir
        except Exception as e:
            logging.error(f"âŒ æå–PDFæ–‡å­—å¤±æ•—: {str(e)}")
            if 'temp_dir' in locals() and temp_dir:
                return None, temp_dir
            return None, None
    
    def preprocess_audio(self, audio_path: str) -> str:
        """é è™•ç†éŸ³é » (åƒ…ç¢ºä¿æ ¼å¼æ­£ç¢º)"""
        logging.info(f"ğŸ”„ é è™•ç†éŸ³é »: {os.path.basename(audio_path)}")
        
        # ç¢ºä¿æª”æ¡ˆç‚º WAV æ ¼å¼
        if not audio_path.lower().endswith('.wav'):
            audio_path = self.convert_to_wav(audio_path)
        
        logging.info(f"âœ… éŸ³é »é è™•ç†å®Œæˆ")
        return audio_path

    def rename_drive_file(self, file_id: str, new_name: str) -> bool:
        """æ ¹æ“šè™•ç†çµæœé‡å‘½å Google Drive ä¸Šçš„æª”æ¡ˆ"""
        try:
            self.drive_service.files().update(
                fileId=file_id,
                body={'name': new_name}
            ).execute()
            logging.info(f"âœ… æˆåŠŸé‡å‘½å Google Drive æª”æ¡ˆ: {new_name}")
            return True
        except Exception as e:
            logging.error(f"âŒ é‡å‘½å Google Drive æª”æ¡ˆå¤±æ•—: {str(e)}")
            return False
        
    def format_timestamp(self, seconds: float) -> str:
            """å°‡ç§’æ•¸è½‰æ›ç‚ºå¯è®€æ™‚é–“æˆ³è¨˜"""
            minutes, seconds = divmod(int(seconds), 60)
            hours, minutes = divmod(minutes, 60)
            
            if hours > 0:
                return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
            else:
                return f"{minutes:02d}:{seconds:02d}"
            
    def extract_date_from_filename(self, filename: str) -> Optional[str]:
        """å¾æª”æ¡ˆåç¨±ä¸­æå–æ—¥æœŸï¼Œæ”¯æ´å¤šç¨®æ ¼å¼"""
        # å˜—è©¦åŒ¹é… REC_YYYYMMDD_HHMMSS æ ¼å¼
        pattern1 = r'REC_(\d{8})_\d+'
        match1 = re.search(pattern1, filename)
        if match1:
            date_str = match1.group(1)
            try:
                # å°‡ YYYYMMDD è½‰æ›ç‚º YYYY-MM-DD
                date_obj = datetime.strptime(date_str, '%Y%m%d')
                return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                pass
        
        # å˜—è©¦åŒ¹é…å·²æœ‰çš„ [YYYY-MM-DD] æ ¼å¼
        pattern2 = r'\[(\d{4}-\d{2}-\d{2})\]'
        match2 = re.search(pattern2, filename)
        if match2:
            return match2.group(1)
            
        # å˜—è©¦åŒ¹é…å…¶ä»–å¯èƒ½çš„æ—¥æœŸæ ¼å¼ (YYYY-MM-DD)
        pattern3 = r'(\d{4}-\d{2}-\d{2})'
        match3 = re.search(pattern3, filename)
        if match3:
            return match3.group(1)
            
        # å¦‚æœéƒ½ç„¡æ³•åŒ¹é…ï¼Œè¿”å› None
        return None

    def try_multiple_gemini_models(self, system_prompt: str, user_content: str, 
                                models: List[str] = None) -> Any:
        """Try generating content using multiple Gemini models until one succeeds.
        
        Args:
            system_prompt: The system instructions for the model
            user_content: The user content to process
            models: List of model names to try in order (uses default list if None)
            
        Returns:
            The successful generation response
            
        Raises:
            Exception: If all models fail
        """
        # Default models list if none provided
        if models is None:
            models = ['gemini-2.5-pro-exp-03-25', 'gemini-2.5-flash-preview-04-17',
                        'gemini-1.5-pro', 'gemini-2.0-flash', 'gemini-1.5-flash', 'gemini-2.0-flash-lite']
        
        response = None
        last_error = None

        # Try different models until successful
        for model_name in models:
            try:
                logging.info(f"ğŸ”„ ä½¿ç”¨æ¨¡å‹: {model_name}")
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(
                    [system_prompt, user_content]
                )
                # If successful, break the loop
                logging.info(f"âœ… æˆåŠŸä½¿ç”¨æ¨¡å‹ {model_name} ç”Ÿæˆç­†è¨˜")
                break
            except Exception as e:
                last_error = e
                # Check if quota error
                if "429" in str(e) or "quota" in str(e).lower():
                    # Extract and log the quota documentation URL
                    url_match = re.search(r'https?://\S+', str(e))
                    logging.warning(f"âš ï¸ æ¨¡å‹ {model_name} é…é¡å·²ç”¨ç›¡: {url_match.group(0)}")
                    # Continue to next model
                    continue
                else:
                    # Raise other errors
                    logging.error(f"âŒ ä½¿ç”¨æ¨¡å‹ {model_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    raise

        # Check if all models failed
        if response is None:
            logging.error("âŒ æ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—äº†")
            raise last_error
        
        return response

    def generate_comprehensive_notes(self, transcript: str) -> str:
        """ä½¿ç”¨ Gemini API ç”Ÿæˆçµæ§‹åŒ–çš„ç­†è¨˜"""
        logging.info("ğŸ”„ ç”Ÿæˆç­†è¨˜...")
        
        try:
            # ä½¿ç”¨æ›´è©³ç´°çš„Markdownæ ¼å¼æŒ‡ç¤º
            system_prompt = """å°‡éŒ„éŸ³é€å­—ç¨¿æ•´ç†æˆç­†è¨˜å…§å®¹ï¼Œè«‹ä½¿ç”¨Markdownæ ¼å¼ç›´æ¥è¼¸å‡ºç­†è¨˜å…§å®¹:
            é¿å…ä½¿ç”¨```markdown```ï¼Œç›´æ¥è¼¸å‡ºMarkdownæ ¼å¼çš„ç­†è¨˜å…§å®¹ã€‚
            """

            # Use the function in generate_comprehensive_notes
            response = self.try_multiple_gemini_models(
                system_prompt,
                f"æœƒè­°é€å­—ç¨¿ï¼š\n{transcript}"
            )
            
            comprehensive_notes = response.text
            logging.info("âœ… ç­†è¨˜ç”ŸæˆæˆåŠŸ")
            return comprehensive_notes
            
        except Exception as e:
            logging.error(f"âŒ ç­†è¨˜ç”Ÿæˆå¤±æ•—: {str(e)}")
            return "ç­†è¨˜ç”Ÿæˆå¤±æ•—ï¼Œè«‹åƒè€ƒæœƒè­°æ‘˜è¦å’Œå®Œæ•´è¨˜éŒ„ã€‚"

    def create_notion_page(self, title: str, summary: str, todos: List[str], segments: List[Dict[str, Any]], speaker_map: Dict[str, str], file_id: str = None) -> Tuple[str, str]:
        """å»ºç«‹å–®ä¸€ Notion é é¢ï¼ŒåŒ…å«æ¨™é¡Œã€æ—¥æœŸã€åƒèˆ‡è€…ã€æ‘˜è¦ã€å¾…è¾¦äº‹é …ã€å®Œæ•´ç­†è¨˜èˆ‡å…§åµŒçš„é€å­—ç¨¿"""
        logging.info("ğŸ”„ å»ºç«‹ Notion é é¢...")

        notion_token = os.getenv("NOTION_TOKEN")
        database_id = os.getenv("NOTION_DATABASE_ID")

        if not notion_token or not database_id:
            raise ValueError("ç¼ºå°‘ Notion API è¨­å®š")

        # --- æº–å‚™é é¢å…§å®¹å€å¡Š ---
        blocks = []
        
        # å¾æª”æ¡ˆåç¨±æå–æ—¥æœŸæˆ–ä½¿ç”¨ç•¶å‰æ—¥æœŸ
        current_date = datetime.now()
        current_date_str = current_date.strftime("%Y-%m-%d")
        
        # å˜—è©¦å¾æª”æ¡ˆIDç²å–æª”æ¡ˆåç¨±ä¸¦æå–æ—¥æœŸ
        file_date = None
        if file_id:
            try:
                file_meta = self.drive_service.files().get(
                    fileId=file_id, fields="name"
                ).execute()
                filename = file_meta.get('name', '')
                if filename:
                    file_date = self.extract_date_from_filename(filename)
            except Exception as e:
                logging.error(f"âŒ ç²å–æª”æ¡ˆæ—¥æœŸå¤±æ•—: {str(e)}")
        
        # ä½¿ç”¨æª”æ¡ˆæ—¥æœŸæˆ–ç•¶å‰æ—¥æœŸ
        date_str = file_date if file_date else current_date_str
        formatted_date = datetime.strptime(date_str, "%Y-%m-%d").strftime("%Yå¹´%mæœˆ%dæ—¥")

        # --- æ¨™é¡Œå€å¡Š (ä½¿ç”¨æ—¥æœŸ+éŒ„éŸ³æª”æ¡ˆåç¨±) ---
        page_title = f"{formatted_date} {title}"

        # --- æ—¥æœŸå€å¡Š ---
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
            "rich_text": [{"type": "text", "text": {"content": "ğŸ“… æ—¥æœŸ"}}]
            }
        })
        blocks.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {
            "rich_text": [{"type": "text", "text": {"content": formatted_date}}]
            }
        })

        blocks.append({"object": "block", "type": "divider", "divider": {}})
        # --- åƒèˆ‡è€…å€å¡Š ---
        participants = list(set(speaker_map.values()))  # ç²å–å”¯ä¸€è­˜åˆ¥çš„åç¨±
        if participants:
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "ğŸ‘¥ åƒèˆ‡è€…"}}]
                }
            })
            participant_text = ", ".join(participants)
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": participant_text}}]
                }
            })
            blocks.append({"object": "block", "type": "divider", "divider": {}})

        # --- æ‘˜è¦å€å¡Š ---
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "ğŸ“ æ‘˜è¦"}}]
            }
        })
        blocks.append({
            "object": "block",
            "type": "callout",
            "callout": {
                "rich_text": [{"type": "text", "text": {"content": summary}}],
                "icon": {"emoji": "ğŸ’¡"}
            }
        })
        blocks.append({"object": "block", "type": "divider", "divider": {}})

        # --- å¾…è¾¦äº‹é …å€å¡Š ---
        if todos:
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "âœ… å¾…è¾¦äº‹é …"}}]
                }
            })
            
            # ä½¿ç”¨ todo list å‘ˆç¾å¾…è¾¦äº‹é …
            todo_blocks = []
            for todo in todos:
                todo_blocks.append({
                    "object": "block",
                    "type": "to_do",
                    "to_do": {
                        "rich_text": [{"type": "text", "text": {"content": todo}}],
                        "checked": False
                    }
                })
            
            blocks.extend(todo_blocks)
            blocks.append({"object": "block", "type": "divider", "divider": {}})

        # --- è™•ç†å®Œæ•´é€å­—ç¨¿ (ç„¡æ™‚é–“æˆ³è¨˜) ---
        full_transcript = ""
        
        for segment in segments:
            speaker = segment["speaker"]
            text = segment["text"]
            content = f"{speaker}: {text}"
            full_transcript += f"{content}\n"

        # --- ç”Ÿæˆå®Œæ•´ç­†è¨˜ ---
        comprehensive_notes = self.generate_comprehensive_notes(full_transcript)
        
        # --- å®Œæ•´ç­†è¨˜å€å¡Š ---
        blocks.append({
            "object": "block",
            "type": "heading_2",
            "heading_2": {
                "rich_text": [{"type": "text", "text": {"content": "ğŸ“Š è©³ç´°ç­†è¨˜"}}]
            }
        })
        
        # Notion API é™åˆ¶ï¼šæ¯æ¬¡è«‹æ±‚æœ€å¤š 100 å€‹å€å¡Š
        MAX_BLOCKS_PER_REQUEST = 90  # ä½¿ç”¨ 90 ä½œç‚ºå®‰å…¨ç•Œé™
        
        # ä½¿ç”¨æ–°çš„ NotionFormatter ä¾†è™•ç†ç­†è¨˜
        note_blocks = self.notion_formatter.process_note_format_for_notion(comprehensive_notes)
        
        # è¨ˆç®—å·²æœ‰å€å¡Šæ•¸
        base_blocks_count = len(blocks)
        available_slots = MAX_BLOCKS_PER_REQUEST - base_blocks_count
        
        # å¦‚æœç­†è¨˜å€å¡Šå¤ªå¤šï¼Œåƒ…æ·»åŠ èƒ½å®¹ç´çš„éƒ¨åˆ†
        if len(note_blocks) > available_slots:
            blocks.extend(note_blocks[:available_slots])
            remaining_note_blocks = note_blocks[available_slots:]
        else:
            blocks.extend(note_blocks)
            remaining_note_blocks = []
        
        headers = {
            "Authorization": f"Bearer {notion_token}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28"
        }

        try:
            # å»ºç«‹ä¸»é é¢ (åŒ…å«æ‰€æœ‰åŸºæœ¬ä¿¡æ¯)
            data = {
                "parent": {"database_id": database_id},
                "properties": {
                    "title": {
                        "title": [{"text": {"content": page_title}}]
                    }
                },
                "children": blocks
            }
            
            logging.info(f"- å»ºç«‹ Notion é é¢ (åŒ…å« {len(blocks)} å€‹å€å¡Šï¼Œé™åˆ¶ç‚º 100)")
            response = requests.post(
                "https://api.notion.com/v1/pages",
                headers=headers,
                json=data
            )
            response.raise_for_status()
            result = response.json()
            page_id = result["id"]
            page_url = result.get("url", f"https://www.notion.so/{page_id.replace('-', '')}")
            
            # å°‡é€å­—ç¨¿åˆ†æˆå¤šå€‹æ®µè½ (å› ç‚º Notion API æœ‰å­—ç¬¦é™åˆ¶)
            remaining_note_blocks.append({"object": "block", "type": "divider", "divider": {}})

            # --- å…§åµŒå®Œæ•´é€å­—ç¨¿å€å¡Š (ä½¿ç”¨ toggle å€å¡Š) ---
            remaining_note_blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {
                    "rich_text": [{"type": "text", "text": {"content": "ğŸ™ï¸ å®Œæ•´é€å­—ç¨¿"}}]
                }
            })
            
            # ä½¿ç”¨ split_transcript_into_blocks ç²å–é€å­—ç¨¿å€å¡Š
            transcript_blocks = self.notion_formatter.split_transcript_into_blocks(full_transcript)
            
            # å»ºç«‹éŸ³é »æª”æ¡ˆé€£çµå€å¡Š
            audio_link_blocks = []
            if file_id:
                try:
                    file_info = self.drive_service.files().get(
                        fileId=file_id, fields="name,webViewLink"
                    ).execute()
                    file_name = file_info.get('name', 'éŸ³é »æª”æ¡ˆ')
                    file_link = file_info.get('webViewLink', f"https://drive.google.com/file/d/{file_id}/view")
                    
                    audio_link_blocks.append({
                        "object": "block",
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": [
                                {"type": "text", "text": {"content": "ğŸ“ éŒ„éŸ³æª”æ¡ˆ: "}},
                                {"type": "text", "text": {"content": file_name, "link": {"url": file_link}}}
                            ]
                        }
                    })
                    audio_link_blocks.append({"object": "block", "type": "divider", "divider": {}})
                except Exception as e:
                    logging.error(f"âŒ ç²å–æª”æ¡ˆé€£çµå¤±æ•—: {str(e)}")
            
            # æ·»åŠ æª”æ¡ˆé€£çµåˆ° remaining_note_blocks
            remaining_note_blocks.extend(audio_link_blocks)
            
            # è¨ˆç®—æ¯å€‹ toggle å€å¡Šæœ€å¤šå¯ä»¥åŒ…å«çš„ transcript_blocks æ•¸é‡ (æœ€å¤§100å€‹)
            MAX_TOGGLE_CHILDREN = 90  # ä¿ç•™ä¸€äº›ç©ºé–“çµ¦å…¶ä»–å…ƒç´ 
            
            # åˆ†å‰² transcript_blocks ç‚ºå¤šå€‹ toggle å€å¡Š
            for i in range(0, len(transcript_blocks), MAX_TOGGLE_CHILDREN):
                toggle_children = []
                end_idx = min(i + MAX_TOGGLE_CHILDREN, len(transcript_blocks))
                
                # åªåœ¨ç¬¬ä¸€å€‹ toggle å€å¡Šæ·»åŠ èªªæ˜æ–‡å­—
                if i == 0:
                    toggle_children.append({
                        "object": "block",
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": [{"type": "text", "text": {"content": "æ­¤å€å¡ŠåŒ…å«å®Œæ•´é€å­—ç¨¿å…§å®¹"}}]
                        }
                    })
                    toggle_children.append({
                        "object": "block",
                        "type": "divider",
                        "divider": {}
                    })
                
                # æ·»åŠ æœ¬æ‰¹æ¬¡çš„ transcript_blocks
                toggle_children.extend(transcript_blocks[i:end_idx])
                
                # å»ºç«‹ç›®å‰æ‰¹æ¬¡çš„ toggle å€å¡Š
                toggle_title = "é»æ“Šå±•é–‹å®Œæ•´é€å­—ç¨¿"
                if i > 0:  # å¦‚æœä¸æ˜¯ç¬¬ä¸€å€‹ toggleï¼Œæ·»åŠ åºè™Ÿ
                    part_num = (i // MAX_TOGGLE_CHILDREN) + 1
                    total_parts = (len(transcript_blocks) + MAX_TOGGLE_CHILDREN - 1) // MAX_TOGGLE_CHILDREN
                    toggle_title = f"é»æ“Šå±•é–‹å®Œæ•´é€å­—ç¨¿ (ç¬¬ {part_num}/{total_parts} éƒ¨åˆ†)"
                
                remaining_note_blocks.append({
                    "object": "block",
                    "type": "toggle",
                    "toggle": {
                        "rich_text": [{"type": "text", "text": {"content": toggle_title}}],
                        "children": toggle_children
                    }
                })
            
            total_batches = (len(remaining_note_blocks) + MAX_BLOCKS_PER_REQUEST - 1) // MAX_BLOCKS_PER_REQUEST
            logging.info(f"- é–‹å§‹åˆ†æ‰¹æ·»åŠ é€å­—ç¨¿å…§å®¹ (å…± {len(remaining_note_blocks)} æ®µï¼Œåˆ† {total_batches} æ‰¹)")

            # Validate Notion token before making requests
            if not notion_token or notion_token.strip() == "":
                raise ValueError("ç„¡æ•ˆçš„ Notion API Token")

            # Add retry logic for batch additions
            for i in range(0, len(remaining_note_blocks), MAX_BLOCKS_PER_REQUEST):
                end_idx = min(i + MAX_BLOCKS_PER_REQUEST, len(remaining_note_blocks))
                batch_num = i // MAX_BLOCKS_PER_REQUEST + 1
                max_retries = 3
                retry_count = 0
                
                while retry_count < max_retries:
                    try:
                        logging.info(f"- æ·»åŠ ç¬¬ {batch_num}/{total_batches} æ‰¹é€å­—ç¨¿å…§å®¹ (ç¬¬ {retry_count + 1} æ¬¡å˜—è©¦)")
                        transcript_blocks_response = requests.patch(
                            f"https://api.notion.com/v1/blocks/{page_id}/children",
                            headers=headers,
                            json={"children": remaining_note_blocks[i:end_idx]},
                            timeout=30  # Add timeout to prevent hanging requests
                        )
                        
                        if transcript_blocks_response.status_code in [401, 403]:
                            logging.error(f"âŒ èªè­‰éŒ¯èª¤ (ç‹€æ…‹ç¢¼: {transcript_blocks_response.status_code}): è«‹ç¢ºèª Notion API Token æœ‰æ•ˆ")
                            try:
                                error_details = transcript_blocks_response.json()
                                logging.error(f"   è©³ç´°éŒ¯èª¤: {json.dumps(error_details, indent=2, ensure_ascii=False)}")
                            except:
                                logging.error(f"   å›æ‡‰å…§å®¹: {transcript_blocks_response.text}")
                            break  # Authentication errors won't be fixed by retrying
                            
                        transcript_blocks_response.raise_for_status()
                        logging.info(f"âœ… ç¬¬ {batch_num}/{total_batches} æ‰¹é€å­—ç¨¿å…§å®¹æ·»åŠ æˆåŠŸ")
                        break  # Success - exit retry loop
                        
                    except requests.exceptions.RequestException as e:
                        retry_count += 1
                        logging.warning(f"âš ï¸ ç¬¬ {batch_num}/{total_batches} æ‰¹é€å­—ç¨¿æ·»åŠ å¤±æ•— (å˜—è©¦ {retry_count}/{max_retries}): {e}")
                        
                        # Check specific error types
                        if hasattr(e, 'response') and e.response is not None:
                            try:
                                err_details = e.response.json()
                                logging.error(f"   éŒ¯èª¤è©³æƒ…: {json.dumps(err_details, indent=2, ensure_ascii=False)}")
                                
                                # Check for specific Notion API errors
                                if 'message' in err_details and 'block contents are invalid' in err_details['message'].lower():
                                    logging.error("   å¯èƒ½å­˜åœ¨ç„¡æ•ˆçš„å€å¡Šå…§å®¹ï¼Œå˜—è©¦ç°¡åŒ–è™•ç†...")
                                    # Simplify blocks if needed in future iterations
                                
                            except json.JSONDecodeError:
                                logging.error(f"   éŸ¿æ‡‰å…§å®¹ (é JSON): {e.response.text}")
                        
                        if retry_count < max_retries:
                            wait_time = 2 ** retry_count  # Exponential backoff
                            logging.info(f"   ç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦...")
                            time.sleep(wait_time)
                        else:
                            logging.error(f"âŒ ç¬¬ {batch_num}/{total_batches} æ‰¹é€å­—ç¨¿æ·»åŠ å¤±æ•—ï¼Œå·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸")
                
                # Add a small delay between batches to avoid rate limiting
                if i + MAX_BLOCKS_PER_REQUEST < len(remaining_note_blocks) and retry_count < max_retries:
                    time.sleep(1)
            
            logging.info(f"âœ… Notion é é¢å»ºç«‹æˆåŠŸ (ID: {page_id}, URL: {page_url})")
            return page_id, page_url
            
        except requests.exceptions.RequestException as e:
            logging.error(f"âŒ Notion API è«‹æ±‚å¤±æ•—: {e}", exc_info=True)
            if e.response is not None:
                try:
                    err_details = e.response.json()
                    logging.error(f"   éŒ¯èª¤ç¢¼: {e.response.status_code}, è¨Šæ¯: {json.dumps(err_details, indent=2, ensure_ascii=False)}")
                except json.JSONDecodeError:
                    logging.error(f"   éŸ¿æ‡‰å…§å®¹ (é JSON): {e.response.text}")
            raise
        except Exception as e:
            logging.error(f"âŒ Notion é é¢å»ºç«‹æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}", exc_info=True)
            raise

    def load_models(self):
        """è¼‰å…¥æ‰€éœ€çš„ AI æ¨¡å‹"""
        logging.info("ğŸ”„ è¼‰å…¥ AI æ¨¡å‹...")
        
        # è¼‰å…¥ Whisper æ¨¡å‹ (å¦‚æœå°šæœªè¼‰å…¥)
        if self.whisper_model is None:
            try:
                logging.info("- è¼‰å…¥ Whisper æ¨¡å‹ (medium)...")
                self.whisper_model = whisper.load_model("medium")
                logging.info("âœ… Whisper æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            except Exception as e:
                logging.error(f"âŒ Whisper æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                raise
        
        # è¼‰å…¥ Pyannote æ¨¡å‹ (å¦‚æœå°šæœªè¼‰å…¥)
        if self.diarization_pipeline is None:
            # å¢åŠ é‡è©¦æ©Ÿåˆ¶
            max_retries = 3
            retry_count = 0
            last_error = None
            
            while retry_count < max_retries:
                try:
                    logging.info(f"- è¼‰å…¥èªªè©±äººåˆ†é›¢æ¨¡å‹... (å˜—è©¦ {retry_count + 1}/{max_retries})")
                    # Make sure we have the HF_TOKEN
                    hf_token = os.getenv("HF_TOKEN")
                    if not hf_token:
                        raise ValueError("Missing HF_TOKEN environment variable")
                    
                    # Using a specific version instead of latest
                    self.diarization_pipeline = Pipeline.from_pretrained(
                        "pyannote/speaker-diarization-3.1",  # Use specific version
                        use_auth_token=hf_token
                    )
                    logging.info("âœ… èªªè©±äººåˆ†é›¢æ¨¡å‹è¼‰å…¥æˆåŠŸ")
                    break
                except Exception as e:
                    last_error = e
                    logging.error(f"âŒ èªªè©±äººåˆ†é›¢æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
                    retry_count += 1
                    time.sleep(2)  # é‡è©¦å‰ç­‰å¾…2ç§’
            
            if self.diarization_pipeline is None:
                logging.error(f"âŒ èªªè©±äººåˆ†é›¢æ¨¡å‹åœ¨ {max_retries} æ¬¡å˜—è©¦å¾Œä»ç„¶è¼‰å…¥å¤±æ•—")
                raise last_error or RuntimeError("Failed to load diarization pipeline")
    
    def convert_to_wav(self, input_path: str) -> str:
        """è½‰æ›æª”æ¡ˆç‚º WAV æ ¼å¼ (16kHz å–®è²é“)"""
        logging.info(f"ğŸ”„ è½‰æ›æª”æ¡ˆæ ¼å¼ç‚º WAV: {os.path.basename(input_path)}")
        
        # ç›®æ¨™è·¯å¾‘ (èˆ‡åŸå§‹æª”æ¡ˆç›¸åŒç›®éŒ„ï¼Œä½†å‰¯æª”åæ”¹ç‚º .wav)
        output_dir = os.path.dirname(input_path)
        output_filename = f"{os.path.splitext(os.path.basename(input_path))[0]}.wav"
        output_path = os.path.join(output_dir, output_filename)
        
        # ä½¿ç”¨ FFmpeg è½‰æ›
        try:
            cmd = [
                "ffmpeg", 
                "-y",                # è¦†è“‹ç¾æœ‰æª”æ¡ˆ
                "-i", input_path,    # è¼¸å…¥æª”æ¡ˆ
                "-ar", "16000",      # æ¡æ¨£ç‡ 16kHz
                "-ac", "1",          # å–®è²é“
                "-c:a", "pcm_s16le", # 16-bit PCM
                output_path          # è¼¸å‡ºæª”æ¡ˆ
            ]
            
            # åŸ·è¡Œå‘½ä»¤
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            logging.info(f"âœ… æª”æ¡ˆè½‰æ›å®Œæˆ: {output_filename}")
            
            return output_path
            
        except subprocess.CalledProcessError as e:
            logging.error(f"âŒ æª”æ¡ˆè½‰æ›å¤±æ•—: {e}")
            raise
    
    def identify_speakers(self, segments: List[Dict[str, Any]], original_speakers: List[str]) -> Dict[str, str]:
        """ä½¿ç”¨ Gemini è¾¨è­˜èªªè©±äººçš„çœŸå¯¦èº«ä»½"""
        logging.info(f"ğŸ”„ è­˜åˆ¥èªªè©±äººèº«ä»½...")
        
        if not segments:
            logging.warning("âš ï¸ æ²’æœ‰èªéŸ³æ®µè½ï¼Œç„¡æ³•è­˜åˆ¥èªªè©±äºº")
            return {}
        
        # æº–å‚™ç¯„ä¾‹å°è©±
        sample_dialogue = ""
        for i, segment in enumerate(segments[:20]):  # æœ€å¤šä½¿ç”¨å‰ 20 å€‹æ®µè½
            speaker = segment["speaker"]
            text = segment["text"]
            sample_dialogue += f"{speaker}: {text}\n"
        
        # æç¤º Gemini è­˜åˆ¥èªªè©±äºº
        try:
            system_prompt = """
            ä½ æ˜¯ä¸€ä½æ–‡å­—è™•ç†å°ˆå®¶ï¼Œå°ˆé–€æ ¹æ“šå°è©±å…§å®¹è¾¨è­˜çœŸå¯¦èªªè©±äººã€‚
            è«‹åˆ†æä»¥ä¸‹å°è©±å…§å®¹ï¼Œè¾¨è­˜å‡ºå„å€‹èªªè©±äººä»£ç¢¼ï¼ˆå¦‚ SPEAKER_00ï¼‰å°æ‡‰çš„æœ€å¯èƒ½çœŸå¯¦å§“åæˆ–è·ç¨±ã€‚
            ä¸ç¢ºå®šçš„èªªè©±äººè«‹ä¿ç•™åŸä»£ç¢¼ã€‚å›æ‡‰æ ¼å¼å¿…é ˆæ˜¯ä¸€å€‹JSONï¼Œkeyç‚ºåŸå§‹èªªè©±äººä»£ç¢¼ï¼Œvalueç‚ºä½ è¾¨è­˜çš„çœŸå¯¦å§“å/è·ç¨±ã€‚
            åªéœ€å›å‚³JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
            """

            response = self.try_multiple_gemini_models(
                system_prompt,
                f"å°è©±å…§å®¹å¦‚ä¸‹ï¼š\n{sample_dialogue}\n\nè«‹è¾¨è­˜å‡ºå„å€‹èªªè©±äººä»£ç¢¼ï¼ˆå¦‚ {', '.join(original_speakers)}ï¼‰å°æ‡‰çš„æœ€å¯èƒ½çœŸå¯¦å§“åæˆ–è·ç¨±ã€‚",
                models=['gemini-2.0-flash', 'gemini-1.5-flash', 'gemini-2.0-flash-lite']
            )
            
            response_text = response.text
            # æœ‰æ™‚ Gemini æœƒåœ¨ JSON å‰å¾ŒåŠ ä¸Šé¡å¤–æ–‡å­—ï¼Œéœ€è¦æå–ç´” JSON éƒ¨åˆ†
            json_match = re.search(r'({.*?})', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            # è§£æ JSON å›æ‡‰
            speaker_map = json.loads(response_text)
            logging.info(f"âœ… èªªè©±äººèº«ä»½è­˜åˆ¥æˆåŠŸ: {speaker_map}")
            
            return speaker_map
            
        except Exception as e:
            logging.error(f"âŒ èªªè©±äººèº«ä»½è­˜åˆ¥å¤±æ•—: {e}")
            return {speaker: speaker for speaker in original_speakers}  # å¤±æ•—æ™‚è¿”å›åŸå§‹ä»£ç¢¼
    
    def generate_summary(self, transcript: str, attachment_text: Optional[str] = None) -> Dict[str, Any]:
        """ä½¿ç”¨ Gemini ç”Ÿæˆæ‘˜è¦ã€æ¨™é¡Œå’Œå¾…è¾¦äº‹é …"""
        logging.info("ğŸ”„ ä½¿ç”¨ Gemini ç”Ÿæˆæ‘˜è¦...")
        
        try:
            context = ""
            if attachment_text:
                context = f"ä»¥ä¸‹æ˜¯æä¾›çš„èƒŒæ™¯è³‡æ–™ï¼š\n{attachment_text}\n\n"
            
            system_prompt = """
            ä½ æ˜¯ä¸€ä½æœƒè­°è¨˜éŒ„å°ˆå®¶ï¼Œå°ˆé•·æ–¼åˆ†ææœƒè­°å…§å®¹ä¸¦ç”¢ç”Ÿé‡é»æ‘˜è¦ã€‚
            è«‹åˆ†æä»¥ä¸‹æœƒè­°è¨˜éŒ„ï¼Œä¸¦æä¾›:
            1. ä¸€å€‹ç°¡çŸ­ä¸”æ¸…æ™°çš„æœƒè­°æ¨™é¡Œ
            2. ä¸€æ®µç°¡æ½”çš„æœƒè­°æ‘˜è¦ (ç´„200-300å­—)
            3. ä¸€å€‹å¾…è¾¦äº‹é …æ¸…å–® (åˆ—å‡ºæœƒè­°ä¸­æåˆ°çš„éœ€è¦åŸ·è¡Œçš„é …ç›®)

            å›æ‡‰æ ¼å¼é ˆç‚º JSONï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
            - title: æœƒè­°æ¨™é¡Œ
            - summary: æœƒè­°æ‘˜è¦
            - todos: å¾…è¾¦äº‹é …æ¸…å–® (é™£åˆ—)

            åªéœ€å›å‚³ JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
            """
            
            response = self.try_multiple_gemini_models(
                system_prompt,
                f"{context}ä»¥ä¸‹æ˜¯æœƒè­°è¨˜éŒ„ï¼š\n{transcript}",
                models=
                ['gemini-2.5-flash-preview-04-17',
                        'gemini-1.5-pro', 'gemini-2.0-flash', 'gemini-1.5-flash', 'gemini-2.0-flash-lite']
            )
            
            response_text = response.text
            # æå– JSON éƒ¨åˆ†
            json_match = re.search(r'({.*?})', response_text, re.DOTALL)
            if json_match:
                response_text = json_match.group(1)
            
            # è§£æ JSON å›æ‡‰
            summary_data = json.loads(response_text)
            logging.info(f"âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸï¼š{summary_data['title']}")
            
            return summary_data
            
        except Exception as e:
            logging.error(f"âŒ æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
            # è¿”å›é è¨­å€¼
            return {
                "title": "æœƒè­°è¨˜éŒ„",
                "summary": "æ‘˜è¦ç”Ÿæˆå¤±æ•—ã€‚" + str(e),
                "todos": ["æª¢æŸ¥æ‘˜è¦ç”Ÿæˆæœå‹™"]
            }

    def process_audio(self, audio_path: str) -> Tuple[str, List[Dict[str, Any]], List[str]]:
        """è™•ç†éŸ³æª”ï¼šé è™•ç†ã€è½‰æ–‡å­—ä¸¦é€²è¡Œèªªè©±äººåˆ†é›¢"""
        logging.info(f"ğŸ”„ è™•ç†éŸ³æª”: {os.path.basename(audio_path)}")
        
        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
        self.load_models()
        
        # å¦‚æœæª”æ¡ˆé WAV æ ¼å¼ï¼Œå…ˆè½‰æ›
        if not audio_path.lower().endswith('.wav'):
            wav_path = self.convert_to_wav(audio_path)
            # ç§»é™¤åŸå§‹æª”æ¡ˆä»¥ç¯€çœç©ºé–“
            os.remove(audio_path)
            audio_path = wav_path
        
        # éŸ³é »é è™•ç† (ç§»é™¤éœéŸ³)
        preprocessed_path = self.preprocess_audio(audio_path)
        if preprocessed_path != audio_path and os.path.exists(audio_path):
            # å¦‚æœç”¢ç”Ÿäº†æ–°çš„è™•ç†æª”æ¡ˆï¼Œå¯ä»¥é¸æ“‡åˆªé™¤åŸå§‹æª”æ¡ˆ
            os.remove(audio_path)
            audio_path = preprocessed_path
            
        # ä½¿ç”¨ Whisper é€²è¡ŒèªéŸ³è½‰æ–‡å­—ï¼Œæ·»åŠ éŒ¯èª¤è™•ç†å’Œå›é€€æ©Ÿåˆ¶
        logging.info("- åŸ·è¡ŒèªéŸ³è½‰æ–‡å­—...")
        asr_result = None
        transcription_attempts = [
            # ç¬¬ä¸€æ¬¡å˜—è©¦ï¼šä½¿ç”¨é è¨­è¨­ç½®
            {"word_timestamps": False, "verbose": False, "model_name": None, "description": "é è¨­è¨­ç½®"},
            # ç¬¬äºŒæ¬¡å˜—è©¦ï¼šä½¿ç”¨è¼ƒå°çš„æ¨¡å‹
            {"word_timestamps": False, "verbose": False, "model_name": "small", "description": "ä½¿ç”¨å°å‹æ¨¡å‹"}
        ]
        
        for i, attempt in enumerate(transcription_attempts):
            try:
                logging.info(f"- å˜—è©¦è½‰éŒ„ ({i+1}/{len(transcription_attempts)}): {attempt['description']}")
                
                # å¦‚æœæŒ‡å®šäº†ä¸åŒçš„æ¨¡å‹å¤§å°ï¼Œè‡¨æ™‚åŠ è¼‰è©²æ¨¡å‹
                temp_model = None
                if attempt['model_name'] and attempt['model_name'] != "medium":
                    temp_model = whisper.load_model(attempt['model_name'])
                    model_to_use = temp_model
                else:
                    model_to_use = self.whisper_model
                
                # åŸ·è¡Œè½‰éŒ„
                asr_result = model_to_use.transcribe(
                    audio_path, 
                    word_timestamps=attempt['word_timestamps'],
                    verbose=attempt['verbose']
                )
                
                # å¦‚æœæˆåŠŸï¼Œé€€å‡ºå˜—è©¦å¾ªç’°
                logging.info(f"âœ… èªéŸ³è½‰éŒ„æˆåŠŸ ({attempt['description']})")
                break
            
            except RuntimeError as e:
                # æª¢æŸ¥æ˜¯å¦æ˜¯å¼µé‡å¤§å°ä¸åŒ¹é…éŒ¯èª¤
                if "must match the size of tensor" in str(e):
                    logging.warning(f"âš ï¸ è½‰éŒ„å¤±æ•— ({attempt['description']}): å¼µé‡å¤§å°ä¸åŒ¹é…ã€‚å˜—è©¦å…¶ä»–åƒæ•¸ã€‚{e}")
                    if i == len(transcription_attempts) - 1:
                        logging.error("âŒ æ‰€æœ‰è½‰éŒ„å˜—è©¦å‡å¤±æ•—")
                    raise
                else:
                    logging.error(f"âŒ è½‰éŒ„æ™‚ç™¼ç”Ÿé‹è¡Œæ™‚éŒ¯èª¤: {e}")
                    raise
            except Exception as e:
                logging.error(f"âŒ è½‰éŒ„å¤±æ•—: {e}")
                raise
        
        if not asr_result:
            raise RuntimeError("ç„¡æ³•è½‰éŒ„éŸ³é »æ–‡ä»¶")
        
        # ä½¿ç”¨ Pyannote é€²è¡Œèªªè©±äººåˆ†é›¢
        logging.info("- åŸ·è¡Œèªªè©±äººåˆ†é›¢...")
        diarization = self.diarization_pipeline(audio_path)
        
        # æ•´åˆçµæœ
        logging.info("- æ•´åˆçµæœ...")
        segments = []
        transcript_full = ""
        original_speakers = set()
        
        # è£½ä½œæ ¼å¼åŒ–çš„è¼¸å‡º
        for i, segment in enumerate(asr_result["segments"]):
            # æ‰¾å‡ºæ­¤æ®µè½çš„ä¸»è¦èªªè©±äºº
            segment_start = segment["start"]
            segment_end = segment["end"]
            text = segment["text"].strip()
            
            # å¾èªªè©±äººåˆ†é›¢çµæœä¸­æ‰¾å‡ºè¦†è“‹æ­¤æ®µè½æ™‚é–“æœ€å¤šçš„èªªè©±äºº
            speakers = {}
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                # è¨ˆç®—é‡ç–Šæ™‚é–“
                overlap_start = max(segment_start, turn.start)
                overlap_end = min(segment_end, turn.end)
                
                if overlap_end > overlap_start:  # æœ‰é‡ç–Š
                    overlap_duration = overlap_end - overlap_start
                    if speaker in speakers:
                        speakers[speaker] += overlap_duration
                    else:
                        speakers[speaker] = overlap_duration
            
            # æ‰¾å‡ºä¸»è¦èªªè©±äºº (è¦†è“‹æ™‚é–“æœ€é•·)
            main_speaker = max(speakers.items(), key=lambda x: x[1])[0] if speakers else "æœªçŸ¥"
            original_speakers.add(main_speaker)
            
            segment_data = {
                "speaker": main_speaker,
                "start": segment_start,
                "end": segment_end,
                "text": text
            }
            
            segments.append(segment_data)
        
        logging.info(f"âœ… éŸ³æª”è™•ç†å®Œæˆï¼Œå…± {len(segments)} å€‹æ®µè½")
        return transcript_full, segments, list(original_speakers)

    def _process_file_job(self, job_id: str, file_id: str, attachment_file_id: Optional[str] = None):
        """å¾Œå°è™•ç†éŸ³é »æª”æ¡ˆçš„å·¥ä½œå‡½æ•¸ (åœ¨ç·šç¨‹ä¸­åŸ·è¡Œ)"""
        audio_temp_dir = None
        attachment_temp_dir = None
        attachment_text = None
        summary_data = None
        speaker_map = {}
        original_filename = None

        # æ›´æ–°å·¥ä½œç‹€æ…‹ç‚ºè™•ç†ä¸­
        with self.jobs_lock:
            self.jobs[job_id]['status'] = JOB_STATUS['PROCESSING']
            self.jobs[job_id]['progress'] = 5
            self.jobs[job_id]['updated_at'] = datetime.now().isoformat()

        try:
            logging.info(f"[Job {job_id}] é–‹å§‹è™•ç† file_id: {file_id}")
            
            # ç²å–åŸå§‹æª”æ¡ˆåç¨±
            try:
                file_meta = self.drive_service.files().get(
                    fileId=file_id, fields="name"
                ).execute()
                original_filename = file_meta.get('name', '')
                logging.info(f"[Job {job_id}] åŸå§‹æª”æ¡ˆåç¨±: {original_filename}")
            except Exception as e:
                logging.error(f"[Job {job_id}] âŒ ç²å–åŸå§‹æª”æ¡ˆåç¨±å¤±æ•—: {e}")
                original_filename = ""
            
            # æ›´æ–°é€²åº¦: 10%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 10
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # è™•ç†é™„ä»¶ (å¦‚æœæœ‰)
            if attachment_file_id:
                attachment_text, attachment_temp_dir = self.download_and_extract_text(attachment_file_id)
            
            # æ›´æ–°é€²åº¦: 20%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 20
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # ä¸‹è¼‰éŸ³é »æª”æ¡ˆ
            audio_path, audio_temp_dir = self.download_from_drive(file_id)
            
            # æ›´æ–°é€²åº¦: 30%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 30
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # è™•ç†éŸ³é »: è½‰éŒ„å’Œèªªè©±äººåˆ†é›¢
            _, segments, original_speakers = self.process_audio(audio_path)
            
            # æ›´æ–°é€²åº¦: 60%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 60
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
                
            # è­˜åˆ¥èªªè©±äºº
            speaker_map = self.identify_speakers(segments, original_speakers)
            
            # æ›´æ–°é€²åº¦: 70%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 70
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # æº–å‚™è¼¸å‡º
            updated_segments = []
            transcript_for_summary = ""
            if not segments:
                logging.warning(f"[Job {job_id}] âš ï¸ No segments found after audio processing. Transcript will be empty.")
            for seg in segments:
                identified_speaker = speaker_map.get(seg['speaker'], seg['speaker'])
                updated_segments.append({**seg, "speaker": identified_speaker})
                transcript_for_summary += f"[{identified_speaker}]: {seg['text']}\n"
            
            # æ›´æ–°é€²åº¦: 75%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 75
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # ç”Ÿæˆæ‘˜è¦
            summary_data = self.generate_summary(transcript_for_summary, attachment_text)
            title = summary_data["title"]
            summary = summary_data["summary"]
            todos = summary_data["todos"]
            
            # æ›´æ–°é€²åº¦: 85%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 85
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # å»ºç«‹ Notion é é¢
            page_id, page_url = self.create_notion_page(
                title, summary, todos, updated_segments, speaker_map, file_id
            )
            
            # æ›´æ–°é€²åº¦: 95%
            with self.jobs_lock:
                self.jobs[job_id]['progress'] = 95
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            # é‡å‘½å Google Drive æª”æ¡ˆ (å¯é¸)
            # å¾åŸå§‹æª”åæå–æ—¥æœŸï¼Œå¦‚æœç„¡æ³•æå–å‰‡ä½¿ç”¨ç•¶å‰æ—¥æœŸ
            file_date = None
            if original_filename:
                file_date = self.extract_date_from_filename(original_filename)
            
            date_str = file_date if file_date else datetime.now().strftime('%Y-%m-%d')
            new_filename = f"[{date_str}] {title}.m4a"
            self.rename_drive_file(file_id, new_filename)
            
            # æ›´æ–°å·¥ä½œç‹€æ…‹ç‚ºå®Œæˆ
            result = {
                "success": True,
                "notion_page_id": page_id,
                "notion_page_url": page_url,
                "title": title,
                "summary": summary,
                "todos": todos,
                "identified_speakers": speaker_map,
                "drive_filename": new_filename
            }
            
            with self.jobs_lock:
                self.jobs[job_id]['status'] = JOB_STATUS['COMPLETED']
                self.jobs[job_id]['progress'] = 100
                self.jobs[job_id]['result'] = result
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            logging.info(f"[Job {job_id}] âœ… è™•ç†å®Œæˆ")
            return result

        except Exception as e:
            logging.error(f"[Job {job_id}] âŒ è™•ç†å¤±æ•—: {e}", exc_info=True)
            
            # æº–å‚™éŒ¯èª¤çµæœ
            final_title = summary_data["title"] if summary_data else "è™•ç†å¤±æ•—"
            final_summary = summary_data["summary"] if summary_data else f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}"
            final_todos = summary_data["todos"] if summary_data else ["æª¢æŸ¥è™•ç†æ—¥èªŒ"]
            final_speakers = speaker_map if speaker_map else None
            
            # æ›´æ–°å·¥ä½œç‹€æ…‹ç‚ºå¤±æ•—
            error_result = {
                "success": False,
                "error": f"è™•ç†å¤±æ•—: {e}",
                "notion_page_id": None,
                "notion_page_url": None,
                "title": final_title,
                "summary": final_summary,
                "todos": final_todos,
                "identified_speakers": final_speakers
            }
            
            with self.jobs_lock:
                self.jobs[job_id]['status'] = JOB_STATUS['FAILED']
                self.jobs[job_id]['progress'] = 100
                self.jobs[job_id]['result'] = error_result
                self.jobs[job_id]['error'] = str(e)
                self.jobs[job_id]['updated_at'] = datetime.now().isoformat()
            
            return error_result

        finally:
            # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
            if audio_temp_dir and os.path.exists(audio_temp_dir):
                logging.info(f"[Job {job_id}] ğŸ§¹ æ¸…ç†éŸ³æª”è‡¨æ™‚ç›®éŒ„")
                shutil.rmtree(audio_temp_dir)
            if attachment_temp_dir and os.path.exists(attachment_temp_dir):
                logging.info(f"[Job {job_id}] ğŸ§¹ æ¸…ç†é™„ä»¶è‡¨æ™‚ç›®éŒ„")
                shutil.rmtree(attachment_temp_dir)

    def process_file_async(self, file_id: str, attachment_file_id: Optional[str] = None) -> str:
        """éåŒæ­¥è™•ç†æª”æ¡ˆï¼Œè¿”å›å·¥ä½œ ID"""
        # ç”Ÿæˆå”¯ä¸€å·¥ä½œ ID
        job_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–å·¥ä½œç‹€æ…‹
        with self.jobs_lock:
            self.jobs[job_id] = {
                'id': job_id,
                'file_id': file_id,
                'attachment_file_id': attachment_file_id,
                'status': JOB_STATUS['PENDING'],
                'progress': 0,
                'created_at': datetime.now().isoformat(),
                'updated_at': datetime.now().isoformat(),
                'result': None,
                'error': None
            }
        
        # æäº¤å·¥ä½œåˆ°ç·šç¨‹æ± 
        self.executor.submit(
            self._process_file_job, job_id, file_id, attachment_file_id
        )
        
        return job_id
    
    def get_job_status(self, job_id: str) -> Dict[str, Any]:
        """ç²å–å·¥ä½œç‹€æ…‹"""
        with self.jobs_lock:
            job = self.jobs.get(job_id)
            
        if not job:
            return {'error': 'å·¥ä½œä¸å­˜åœ¨'}
        
        # æ ¹æ“šå·¥ä½œç‹€æ…‹è¿”å›ä¸åŒä¿¡æ¯
        if job['status'] == JOB_STATUS['COMPLETED']:
            return {
                'id': job['id'],
                'status': job['status'],
                'progress': job['progress'],
                'created_at': job['created_at'],
                'updated_at': job['updated_at'],
                'result': job['result']
            }
        elif job['status'] == JOB_STATUS['FAILED']:
            return {
                'id': job['id'],
                'status': job['status'],
                'progress': job['progress'],
                'created_at': job['created_at'],
                'updated_at': job['updated_at'],
                'error': job['error']
            }
        else:
            # è™•ç†ä¸­æˆ–ç­‰å¾…ä¸­
            return {
                'id': job['id'],
                'status': job['status'],
                'progress': job['progress'],
                'created_at': job['created_at'],
                'updated_at': job['updated_at']
            }

# åˆå§‹åŒ–è™•ç†å™¨
processor = AudioProcessor(max_workers=3)

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    # Create a consistent snapshot of jobs while holding the lock
    with processor.jobs_lock:
        # Create a full copy of the jobs dictionary to ensure a consistent snapshot
        all_jobs = {job_id: job.copy() for job_id, job in processor.jobs.items()}
    
    # Count active jobs from the snapshot (outside the lock)
    active_job_count = len([j for j in all_jobs.values() 
                          if j['status'] in [JOB_STATUS['PENDING'], JOB_STATUS['PROCESSING']]])
    
    # Log the count for debugging
    logging.debug(f"Health check: Found {active_job_count} active jobs at {datetime.now().isoformat()}")
    
    return jsonify({
        "status": "healthy", 
        "timestamp": datetime.now().isoformat(),
        "active_jobs": active_job_count
    })

@app.route('/process', methods=['POST'])
def process_audio_endpoint():
    """éåŒæ­¥è™•ç†éŸ³æª”çš„ API ç«¯é»ï¼Œç«‹å³è¿”å›å·¥ä½œ ID"""
    try:
        data = request.json

        if not data:
            return jsonify({"success": False, "error": "ç„¡æ•ˆçš„è«‹æ±‚å…§å®¹"}), 400

        file_id = data.get('file_id')
        attachment_file_id = data.get('attachment_file_id')

        if not file_id:
            return jsonify({"success": False, "error": "ç¼ºå°‘ file_id åƒæ•¸"}), 400

        # å‰µå»ºéåŒæ­¥å·¥ä½œ
        job_id = processor.process_file_async(file_id, attachment_file_id)
        
        # ç«‹å³è¿”å›å·¥ä½œID
        return jsonify({
            "success": True,
            "message": "å·¥ä½œå·²æäº¤ï¼Œæ­£åœ¨å¾Œå°è™•ç†",
            "job_id": job_id
        })

    except Exception as e:
        logging.error(f"API éŒ¯èª¤: {e}", exc_info=True)
        return jsonify({"success": False, "error": f"ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {e}"}), 500

@app.route('/job/<job_id>', methods=['GET'])
def get_job_status_endpoint(job_id):
    """ç²å–å·¥ä½œç‹€æ…‹çš„ API ç«¯é»"""
    try:
        job_status = processor.get_job_status(job_id)
        
        if 'error' in job_status:
            return jsonify({"success": False, "error": job_status['error']}), 404
            
        return jsonify({
            "success": True,
            "job": job_status
        })
        
    except Exception as e:
        logging.error(f"API éŒ¯èª¤: {e}", exc_info=True)
        return jsonify({"success": False, "error": f"ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {e}"}), 500

@app.route('/jobs', methods=['GET'])
def get_active_jobs_endpoint():
    """ç²å–å·¥ä½œåˆ—è¡¨çš„ API ç«¯é»ï¼Œå¯é¸æ“‡æ€§éæ¿¾ç‹€æ…‹"""
    try:
        # Get filter status from query parameter, default to show only active jobs
        filter_status = request.args.get('filter', 'active')
        
        # Create a consistent snapshot of jobs while holding the lock
        with processor.jobs_lock:
            # First create a snapshot of all jobs while holding the lock
            all_jobs = {job_id: job.copy() for job_id, job in processor.jobs.items()}
        
        # Process the jobs data outside the lock to minimize lock contention
        if filter_status == 'all':
            # Return all jobs regardless of status
            jobs_to_return = {
                job_id: {
                    'id': job['id'],
                    'status': job['status'],
                    'progress': job['progress'],
                    'created_at': job['created_at'],
                    'updated_at': job['updated_at']
                }
                for job_id, job in all_jobs.items()
            }
        elif filter_status == 'active':
            # Return only pending or processing jobs
            jobs_to_return = {
                job_id: {
                    'id': job['id'],
                    'status': job['status'],
                    'progress': job['progress'],
                    'created_at': job['created_at'],
                    'updated_at': job['updated_at']
                }
                for job_id, job in all_jobs.items()
                if job['status'] in [JOB_STATUS['PENDING'], JOB_STATUS['PROCESSING']]
            }
        elif filter_status == 'completed':
            # Return only completed jobs
            jobs_to_return = {
                job_id: {
                    'id': job['id'],
                    'status': job['status'],
                    'progress': job['progress'],
                    'created_at': job['created_at'],
                    'updated_at': job['updated_at']
                }
                for job_id, job in all_jobs.items()
                if job['status'] == JOB_STATUS['COMPLETED']
            }
        elif filter_status == 'failed':
            # Return only failed jobs
            jobs_to_return = {
                job_id: {
                    'id': job['id'],
                    'status': job['status'],
                    'progress': job['progress'],
                    'created_at': job['created_at'],
                    'updated_at': job['updated_at']
                }
                for job_id, job in all_jobs.items()
                if job['status'] == JOB_STATUS['FAILED']
            }
        else:
            # Invalid filter value
            return jsonify({"success": False, "error": "Invalid filter parameter. Use 'active', 'all', 'completed', or 'failed'"}), 400
            
        # Add job count information
        result = {
            "success": True,
            "active_jobs": jobs_to_return,
            "count": len(jobs_to_return),
            "timestamp": datetime.now().isoformat()
        }
        
        # Log the results for debugging
        logging.debug(f"Jobs endpoint: Found {len(jobs_to_return)} jobs with filter={filter_status} at {datetime.now().isoformat()}")
        
        return jsonify(result)
        
    except Exception as e:
        logging.error(f"API éŒ¯èª¤: {e}", exc_info=True)
        return jsonify({"success": False, "error": f"ä¼ºæœå™¨å…§éƒ¨éŒ¯èª¤: {e}"}), 500

# ===== å‰ç«¯è·¯ç”± =====
@app.route('/')
def index():
    """ä¸»é é¢"""
    return render_template('index.html')

@app.route('/login')
def login():
    """ç™»å…¥é é¢"""
    return render_template('login.html')

@app.route('/callback')
def callback():
    """OAuthå›èª¿é é¢"""
    return render_template('callback.html')

# ===== é©—è­‰API =====
@app.route('/api/auth/google')
def auth_google():
    """é‡å®šå‘åˆ° Google OAuth"""
    # æ³¨æ„: é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„OAuthè¨­ç½®å¯¦ç¾
    # é€™æ˜¯ä¸€å€‹ç°¡åŒ–çš„ç¤ºä¾‹
    return redirect('/api/auth/callback')

@app.route('/api/auth/google/login')
def auth_google_login():
    """Google ç™»å…¥å…¥å£é»"""
    # èˆ‡ auth_google åŠŸèƒ½ç›¸åŒï¼Œç‚ºäº†å…¼å®¹æ€§æä¾›
    return redirect('/api/auth/google')

@app.route('/api/auth/callback')
def auth_callback():
    """è™•ç†OAuthå›èª¿"""
    # æ³¨æ„: é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„OAuthæµç¨‹å¯¦ç¾
    code = request.args.get('code')
    if code:
        # è™•ç†æˆæ¬Šç¢¼ï¼Œç²å–tokenç­‰æ“ä½œ...
        session['authenticated'] = True
        return redirect('/callback')
    else:
        return redirect('/login?error=No+authorization+code+received')

@app.route('/api/auth/token', methods=['POST'])
def auth_token():
    """å°‡æˆæ¬Šç¢¼è½‰æ›ç‚ºä»¤ç‰Œ"""
    data = request.json
    code = data.get('code')
    
    if not code:
        return jsonify({'success': False, 'error': 'No authorization code provided'})
    
    # å¯¦éš›æ‡‰ç”¨ä¸­é€™è£¡éœ€è¦èˆ‡OAuthæä¾›å•†äº¤æ›ä»¤ç‰Œ
    # ç‚ºäº†ç¤ºä¾‹ï¼Œæˆ‘å€‘ç°¡å–®åœ°è¨­ç½®session
    session['authenticated'] = True
    
    return jsonify({'success': True})

@app.route('/api/auth/status')
def auth_status():
    """æª¢æŸ¥ç”¨æˆ¶èªè­‰ç‹€æ…‹"""
    authenticated = session.get('authenticated', False)
    
    if authenticated:
        # ç°¡åŒ–ç¤ºä¾‹ï¼šå¯¦éš›æ‡‰ç”¨éœ€è¦å¾OAuth tokenç²å–ç”¨æˆ¶è³‡è¨Š
        user = {
            'id': '12345',
            'name': 'ç¤ºä¾‹ç”¨æˆ¶',
            'email': 'user@example.com',
            'picture': 'https://via.placeholder.com/150'
        }
        return jsonify({
            'authenticated': True,
            'user': user
        })
    else:
        return jsonify({
            'authenticated': False
        })

@app.route('/api/auth/logout', methods=['POST'])
def auth_logout():
    """ç™»å‡ºç”¨æˆ¶"""
    session.clear()
    return jsonify({'success': True})

@app.route('/drive/files')
def drive_files():
    """ç²å–Google Driveæª”æ¡ˆåˆ—è¡¨"""
    # æª¢æŸ¥èªè­‰
    if not session.get('authenticated', False):
        return jsonify({
            'success': False,
            'error': 'Not authenticated'
        }), 401
    
    # æ³¨æ„: å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰èª¿ç”¨Drive APIç²å–æª”æ¡ˆ
    # é€™è£¡æä¾›ç¤ºä¾‹è³‡æ–™ç”¨æ–¼å‰ç«¯é–‹ç™¼
    audio_files = [
        {
            'id': 'audio1',
            'name': 'æœƒè­°éŒ„éŸ³ 2023-05-01.m4a',
            'mimeType': 'audio/x-m4a',
            'size': 12582912  # 12 MB
        },
        {
            'id': 'audio2',
            'name': 'REC_20230510_142355.wav',
            'mimeType': 'audio/wav',
            'size': 31457280  # 30 MB
        },
        {
            'id': 'audio3',
            'name': 'éŠ·å”®æœƒè­°è¨è«–.mp3',
            'mimeType': 'audio/mp3',
            'size': 8388608  # 8 MB
        }
    ]
    
    pdf_files = [
        {
            'id': 'pdf1',
            'name': 'æœƒè­°è­°ç¨‹.pdf',
            'mimeType': 'application/pdf',
            'size': 1048576  # 1 MB
        },
        {
            'id': 'pdf2',
            'name': 'å­£åº¦å ±å‘Š.pdf',
            'mimeType': 'application/pdf',
            'size': 3145728  # 3 MB
        }
    ]
    
    files = audio_files + pdf_files
    
    return jsonify({
        'success': True,
        'files': files
    })

if __name__ == "__main__":
    port = int(os.getenv("PORT", 5000))
    debug = os.getenv("FLASK_DEBUG", "false").lower() == "true"
    
    logging.info(f"ğŸš€ å•Ÿå‹•ä¼ºæœå™¨æ–¼ port {port}...")
    app.run(host='0.0.0.0', port=port, debug=debug, threaded=True)
